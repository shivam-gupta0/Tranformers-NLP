{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Refrence**\n",
        "1. https://arxiv.org/abs/1706.03762\n",
        "2. https://www.tensorflow.org/text/tutorials/transformer"
      ],
      "metadata": {
        "id": "JtoszeIdh-el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Connect Drive**\n"
      ],
      "metadata": {
        "id": "zxzY8ZEk3FX7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50aWxz6C2wyP",
        "outputId": "e3cb9d4c-07c4-4041-8ef0-1d4f896500b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set wroking directory**"
      ],
      "metadata": {
        "id": "c_JAztUF3KtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Transformer_NLP_Translator/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUacygIR3K0_",
        "outputId": "c793c17f-8bc9-49e9-d7ae-3987a0f519f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Transformer_NLP_Translator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time # to see how long it takes in training\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds # tools for the tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2t4IhTa30od",
        "outputId": "805b8355-4cb4-41af-803c-da8df1e792d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data**"
      ],
      "metadata": {
        "id": "1dN62fsq3p-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.statmt.org/europarl/\n",
        "\n",
        "https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes"
      ],
      "metadata": {
        "id": "oNUqTBpciw5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "dp-1M9Cq4LMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"europarl-v7.de-en.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_en = f.read()\n",
        "\n",
        "with open(\"europarl-v7.de-en.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_de = f.read()\n",
        "\n",
        "print(text_en[:100])\n",
        "print(text_de[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0xEjiyz3qGP",
        "outputId": "5ea12cc6-ef55-42de-de31-4127584eb929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumption of the session\n",
            "I declare resumed the session of the European Parliament adjourned on Frid\n",
            "Wiederaufnahme der Sitzungsperiode\n",
            "Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"nonbreaking_prefix.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_en = \"\"\n",
        "    for ln in f:\n",
        "        if ln.startswith(\"#\"):\n",
        "            continue\n",
        "        else:\n",
        "          non_breaking_prefix_en += ln\n",
        "\n",
        "with open(\"nonbreaking_prefix.de\", mode='r', encoding=\"utf-8\") as fe:\n",
        "    non_breaking_prefix_de = \"\"\n",
        "    for ln in fe:\n",
        "        if ln.startswith(\"#\") and ln.islower():\n",
        "            continue\n",
        "        else:\n",
        "          non_breaking_prefix_de += ln\n",
        "#non_breaking_prefix_en = non_breaking_prefix_en[369:]\n",
        "#non_breaking_prefix_de= non_breaking_prefix_de[369:]\n",
        "print(non_breaking_prefix_en)\n",
        "print(non_breaking_prefix_de)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvySz4mm4Opp",
        "outputId": "3d4110de-9335-42ee-f0fa-61b25691f4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "E\n",
            "F\n",
            "G\n",
            "H\n",
            "I\n",
            "J\n",
            "K\n",
            "L\n",
            "M\n",
            "N\n",
            "O\n",
            "P\n",
            "Q\n",
            "R\n",
            "S\n",
            "T\n",
            "U\n",
            "V\n",
            "W\n",
            "X\n",
            "Y\n",
            "Z\n",
            "\n",
            "Adj\n",
            "Adm\n",
            "Adv\n",
            "Asst\n",
            "Bart\n",
            "Bldg\n",
            "Brig\n",
            "Bros\n",
            "Capt\n",
            "Cmdr\n",
            "Col\n",
            "Comdr\n",
            "Con\n",
            "Corp\n",
            "Cpl\n",
            "DR\n",
            "Dr\n",
            "Drs\n",
            "Ens\n",
            "Gen\n",
            "Gov\n",
            "Hon\n",
            "Hr\n",
            "Hosp\n",
            "Insp\n",
            "Lt\n",
            "MM\n",
            "MR\n",
            "MRS\n",
            "MS\n",
            "Maj\n",
            "Messrs\n",
            "Mlle\n",
            "Mme\n",
            "Mr\n",
            "Mrs\n",
            "Ms\n",
            "Msgr\n",
            "Op\n",
            "Ord\n",
            "Pfc\n",
            "Ph\n",
            "Prof\n",
            "Pvt\n",
            "Rep\n",
            "Reps\n",
            "Res\n",
            "Rev\n",
            "Rt\n",
            "Sen\n",
            "Sens\n",
            "Sfc\n",
            "Sgt\n",
            "Sr\n",
            "St\n",
            "Supt\n",
            "Surg\n",
            "\n",
            "v\n",
            "vs\n",
            "i.e\n",
            "rev\n",
            "e.g\n",
            "Rs\n",
            "\n",
            "No #NUMERIC_ONLY# \n",
            "Nos\n",
            "Art #NUMERIC_ONLY#\n",
            "Nr\n",
            "pp #NUMERIC_ONLY#\n",
            "\n",
            "Jan\n",
            "Feb\n",
            "Mar\n",
            "Apr\n",
            "Jun\n",
            "Jul\n",
            "Aug\n",
            "Sep\n",
            "Oct\n",
            "Nov\n",
            "Dec\n",
            "#Anything in this file, followed by a period (and an upper-case word), does NOT indicate an end-of-sentence marker.\n",
            "#Special cases are included for prefixes that ONLY appear before 0-9 numbers.\n",
            "\n",
            "#any single upper case letter  followed by a period is not a sentence ender (excluding I occasionally, but we leave it in)\n",
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "E\n",
            "F\n",
            "G\n",
            "H\n",
            "I\n",
            "J\n",
            "K\n",
            "L\n",
            "M\n",
            "N\n",
            "O\n",
            "P\n",
            "Q\n",
            "R\n",
            "S\n",
            "T\n",
            "U\n",
            "V\n",
            "W\n",
            "X\n",
            "Y\n",
            "Z\n",
            "a\n",
            "b\n",
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n",
            "k\n",
            "l\n",
            "m\n",
            "n\n",
            "o\n",
            "p\n",
            "q\n",
            "r\n",
            "s\n",
            "t\n",
            "u\n",
            "v\n",
            "w\n",
            "x\n",
            "y\n",
            "z\n",
            "\n",
            "\n",
            "#Roman Numerals. A dot after one of these is not a sentence break in German.\n",
            "I\n",
            "II\n",
            "III\n",
            "IV\n",
            "V\n",
            "VI\n",
            "VII\n",
            "VIII\n",
            "IX\n",
            "X\n",
            "XI\n",
            "XII\n",
            "XIII\n",
            "XIV\n",
            "XV\n",
            "XVI\n",
            "XVII\n",
            "XVIII\n",
            "XIX\n",
            "XX\n",
            "i\n",
            "ii\n",
            "iii\n",
            "iv\n",
            "v\n",
            "vi\n",
            "vii\n",
            "viii\n",
            "ix\n",
            "x\n",
            "xi\n",
            "xii\n",
            "xiii\n",
            "xiv\n",
            "xv\n",
            "xvi\n",
            "xvii\n",
            "xviii\n",
            "xix\n",
            "xx\n",
            "\n",
            "#Titles and Honorifics\n",
            "Adj\n",
            "Adm\n",
            "Adv\n",
            "Asst\n",
            "Bart\n",
            "Bldg\n",
            "Brig\n",
            "Bros\n",
            "Capt\n",
            "Cmdr\n",
            "Col\n",
            "Comdr\n",
            "Con\n",
            "Corp\n",
            "Cpl\n",
            "DR\n",
            "Dr\n",
            "Ens\n",
            "Gen\n",
            "Gov\n",
            "Hon\n",
            "Hosp\n",
            "Insp\n",
            "Lt\n",
            "MM\n",
            "MR\n",
            "MRS\n",
            "MS\n",
            "Maj\n",
            "Messrs\n",
            "Mlle\n",
            "Mme\n",
            "Mr\n",
            "Mrs\n",
            "Ms\n",
            "Msgr\n",
            "Op\n",
            "Ord\n",
            "Pfc\n",
            "Ph\n",
            "Prof\n",
            "Pvt\n",
            "Rep\n",
            "Reps\n",
            "Res\n",
            "Rev\n",
            "Rt\n",
            "Sen\n",
            "Sens\n",
            "Sfc\n",
            "Sgt\n",
            "Sr\n",
            "St\n",
            "Supt\n",
            "Surg\n",
            "\n",
            "#Misc symbols\n",
            "Mio\n",
            "Mrd\n",
            "bzw\n",
            "v\n",
            "vs\n",
            "usw\n",
            "d.h\n",
            "z.B\n",
            "u.a\n",
            "etc\n",
            "Mrd\n",
            "MwSt\n",
            "ggf\n",
            "d.J\n",
            "D.h\n",
            "m.E\n",
            "vgl\n",
            "I.F\n",
            "z.T\n",
            "sogen\n",
            "ff\n",
            "u.E\n",
            "g.U\n",
            "g.g.A\n",
            "c.-à-d\n",
            "Buchst\n",
            "u.s.w\n",
            "sog\n",
            "u.ä\n",
            "Std\n",
            "evtl\n",
            "Zt\n",
            "Chr\n",
            "u.U\n",
            "o.ä\n",
            "Ltd\n",
            "b.A\n",
            "z.Zt\n",
            "spp\n",
            "sen\n",
            "SA\n",
            "k.o\n",
            "jun\n",
            "i.H.v\n",
            "dgl\n",
            "dergl\n",
            "Co\n",
            "zzt\n",
            "usf\n",
            "s.p.a\n",
            "Dkr\n",
            "Corp\n",
            "bzgl\n",
            "BSE\n",
            "\n",
            "#Number indicators\n",
            "# add #NUMERIC_ONLY# after the word if it should ONLY be non-breaking when a 0-9 digit follows it\n",
            "No\n",
            "Nos\n",
            "Art\n",
            "Nr\n",
            "pp\n",
            "ca\n",
            "Ca\n",
            "\n",
            "#Ordinals are done with . in German - \"1.\" = \"1st\" in English\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data cleaning**"
      ],
      "metadata": {
        "id": "nx1SOcmx4WSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_breaking_prefix_e = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_e = [' ' + pref.lower() + '.' for pref in non_breaking_prefix_en if pref != \"\\n\"]\n",
        "non_breaking_prefix_d = non_breaking_prefix_de.split(\"\\n\")\n",
        "non_breaking_prefix_d = [' ' + pre.lower() + '.' for pre in non_breaking_prefix_de if pre != \"\\n\"]\n",
        "print(non_breaking_prefix_e)\n",
        "print(non_breaking_prefix_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heLANt4J4ZBL",
        "outputId": "f80eeb33-e3e6-4fe5-f3f3-c529eb291954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' a.', ' b.', ' c.', ' d.', ' e.', ' f.', ' g.', ' h.', ' i.', ' j.', ' k.', ' l.', ' m.', ' n.', ' o.', ' p.', ' q.', ' r.', ' s.', ' t.', ' u.', ' v.', ' w.', ' x.', ' y.', ' z.', ' a.', ' d.', ' j.', ' a.', ' d.', ' m.', ' a.', ' d.', ' v.', ' a.', ' s.', ' s.', ' t.', ' b.', ' a.', ' r.', ' t.', ' b.', ' l.', ' d.', ' g.', ' b.', ' r.', ' i.', ' g.', ' b.', ' r.', ' o.', ' s.', ' c.', ' a.', ' p.', ' t.', ' c.', ' m.', ' d.', ' r.', ' c.', ' o.', ' l.', ' c.', ' o.', ' m.', ' d.', ' r.', ' c.', ' o.', ' n.', ' c.', ' o.', ' r.', ' p.', ' c.', ' p.', ' l.', ' d.', ' r.', ' d.', ' r.', ' d.', ' r.', ' s.', ' e.', ' n.', ' s.', ' g.', ' e.', ' n.', ' g.', ' o.', ' v.', ' h.', ' o.', ' n.', ' h.', ' r.', ' h.', ' o.', ' s.', ' p.', ' i.', ' n.', ' s.', ' p.', ' l.', ' t.', ' m.', ' m.', ' m.', ' r.', ' m.', ' r.', ' s.', ' m.', ' s.', ' m.', ' a.', ' j.', ' m.', ' e.', ' s.', ' s.', ' r.', ' s.', ' m.', ' l.', ' l.', ' e.', ' m.', ' m.', ' e.', ' m.', ' r.', ' m.', ' r.', ' s.', ' m.', ' s.', ' m.', ' s.', ' g.', ' r.', ' o.', ' p.', ' o.', ' r.', ' d.', ' p.', ' f.', ' c.', ' p.', ' h.', ' p.', ' r.', ' o.', ' f.', ' p.', ' v.', ' t.', ' r.', ' e.', ' p.', ' r.', ' e.', ' p.', ' s.', ' r.', ' e.', ' s.', ' r.', ' e.', ' v.', ' r.', ' t.', ' s.', ' e.', ' n.', ' s.', ' e.', ' n.', ' s.', ' s.', ' f.', ' c.', ' s.', ' g.', ' t.', ' s.', ' r.', ' s.', ' t.', ' s.', ' u.', ' p.', ' t.', ' s.', ' u.', ' r.', ' g.', ' v.', ' v.', ' s.', ' i.', ' ..', ' e.', ' r.', ' e.', ' v.', ' e.', ' ..', ' g.', ' r.', ' s.', ' n.', ' o.', '  .', ' #.', ' n.', ' u.', ' m.', ' e.', ' r.', ' i.', ' c.', ' _.', ' o.', ' n.', ' l.', ' y.', ' #.', '  .', ' n.', ' o.', ' s.', ' a.', ' r.', ' t.', '  .', ' #.', ' n.', ' u.', ' m.', ' e.', ' r.', ' i.', ' c.', ' _.', ' o.', ' n.', ' l.', ' y.', ' #.', ' n.', ' r.', ' p.', ' p.', '  .', ' #.', ' n.', ' u.', ' m.', ' e.', ' r.', ' i.', ' c.', ' _.', ' o.', ' n.', ' l.', ' y.', ' #.', ' j.', ' a.', ' n.', ' f.', ' e.', ' b.', ' m.', ' a.', ' r.', ' a.', ' p.', ' r.', ' j.', ' u.', ' n.', ' j.', ' u.', ' l.', ' a.', ' u.', ' g.', ' s.', ' e.', ' p.', ' o.', ' c.', ' t.', ' n.', ' o.', ' v.', ' d.', ' e.', ' c.']\n",
            "[' a.', ' b.', ' c.', ' d.', ' e.', ' f.', ' g.', ' h.', ' i.', ' j.', ' k.', ' l.', ' m.', ' n.', ' o.', ' p.', ' q.', ' r.', ' s.', ' t.', ' u.', ' v.', ' w.', ' x.', ' y.', ' z.', ' a.', ' b.', ' c.', ' d.', ' e.', ' f.', ' g.', ' h.', ' i.', ' j.', ' k.', ' l.', ' m.', ' n.', ' o.', ' p.', ' q.', ' r.', ' s.', ' t.', ' u.', ' v.', ' w.', ' x.', ' y.', ' z.', ' i.', ' i.', ' i.', ' i.', ' i.', ' i.', ' i.', ' v.', ' v.', ' v.', ' i.', ' v.', ' i.', ' i.', ' v.', ' i.', ' i.', ' i.', ' i.', ' x.', ' x.', ' x.', ' i.', ' x.', ' i.', ' i.', ' x.', ' i.', ' i.', ' i.', ' x.', ' i.', ' v.', ' x.', ' v.', ' x.', ' v.', ' i.', ' x.', ' v.', ' i.', ' i.', ' x.', ' v.', ' i.', ' i.', ' i.', ' x.', ' i.', ' x.', ' x.', ' x.', ' i.', ' i.', ' i.', ' i.', ' i.', ' i.', ' i.', ' v.', ' v.', ' v.', ' i.', ' v.', ' i.', ' i.', ' v.', ' i.', ' i.', ' i.', ' i.', ' x.', ' x.', ' x.', ' i.', ' x.', ' i.', ' i.', ' x.', ' i.', ' i.', ' i.', ' x.', ' i.', ' v.', ' x.', ' v.', ' x.', ' v.', ' i.', ' x.', ' v.', ' i.', ' i.', ' x.', ' v.', ' i.', ' i.', ' i.', ' x.', ' i.', ' x.', ' x.', ' x.', ' a.', ' d.', ' j.', ' a.', ' d.', ' m.', ' a.', ' d.', ' v.', ' a.', ' s.', ' s.', ' t.', ' b.', ' a.', ' r.', ' t.', ' b.', ' l.', ' d.', ' g.', ' b.', ' r.', ' i.', ' g.', ' b.', ' r.', ' o.', ' s.', ' c.', ' a.', ' p.', ' t.', ' c.', ' m.', ' d.', ' r.', ' c.', ' o.', ' l.', ' c.', ' o.', ' m.', ' d.', ' r.', ' c.', ' o.', ' n.', ' c.', ' o.', ' r.', ' p.', ' c.', ' p.', ' l.', ' d.', ' r.', ' d.', ' r.', ' e.', ' n.', ' s.', ' g.', ' e.', ' n.', ' g.', ' o.', ' v.', ' h.', ' o.', ' n.', ' h.', ' o.', ' s.', ' p.', ' i.', ' n.', ' s.', ' p.', ' l.', ' t.', ' m.', ' m.', ' m.', ' r.', ' m.', ' r.', ' s.', ' m.', ' s.', ' m.', ' a.', ' j.', ' m.', ' e.', ' s.', ' s.', ' r.', ' s.', ' m.', ' l.', ' l.', ' e.', ' m.', ' m.', ' e.', ' m.', ' r.', ' m.', ' r.', ' s.', ' m.', ' s.', ' m.', ' s.', ' g.', ' r.', ' o.', ' p.', ' o.', ' r.', ' d.', ' p.', ' f.', ' c.', ' p.', ' h.', ' p.', ' r.', ' o.', ' f.', ' p.', ' v.', ' t.', ' r.', ' e.', ' p.', ' r.', ' e.', ' p.', ' s.', ' r.', ' e.', ' s.', ' r.', ' e.', ' v.', ' r.', ' t.', ' s.', ' e.', ' n.', ' s.', ' e.', ' n.', ' s.', ' s.', ' f.', ' c.', ' s.', ' g.', ' t.', ' s.', ' r.', ' s.', ' t.', ' s.', ' u.', ' p.', ' t.', ' s.', ' u.', ' r.', ' g.', ' m.', ' i.', ' o.', ' m.', ' r.', ' d.', ' b.', ' z.', ' w.', ' v.', ' v.', ' s.', ' u.', ' s.', ' w.', ' d.', ' ..', ' h.', ' z.', ' ..', ' b.', ' u.', ' ..', ' a.', ' e.', ' t.', ' c.', ' m.', ' r.', ' d.', ' m.', ' w.', ' s.', ' t.', ' g.', ' g.', ' f.', ' d.', ' ..', ' j.', ' d.', ' ..', ' h.', ' m.', ' ..', ' e.', ' v.', ' g.', ' l.', ' i.', ' ..', ' f.', ' z.', ' ..', ' t.', ' s.', ' o.', ' g.', ' e.', ' n.', ' f.', ' f.', ' u.', ' ..', ' e.', ' g.', ' ..', ' u.', ' g.', ' ..', ' g.', ' ..', ' a.', ' c.', ' ..', ' -.', ' à.', ' -.', ' d.', ' b.', ' u.', ' c.', ' h.', ' s.', ' t.', ' u.', ' ..', ' s.', ' ..', ' w.', ' s.', ' o.', ' g.', ' u.', ' ..', ' ä.', ' s.', ' t.', ' d.', ' e.', ' v.', ' t.', ' l.', ' z.', ' t.', ' c.', ' h.', ' r.', ' u.', ' ..', ' u.', ' o.', ' ..', ' ä.', ' l.', ' t.', ' d.', ' b.', ' ..', ' a.', ' z.', ' ..', ' z.', ' t.', ' s.', ' p.', ' p.', ' s.', ' e.', ' n.', ' s.', ' a.', ' k.', ' ..', ' o.', ' j.', ' u.', ' n.', ' i.', ' ..', ' h.', ' ..', ' v.', ' d.', ' g.', ' l.', ' d.', ' e.', ' r.', ' g.', ' l.', ' c.', ' o.', ' z.', ' z.', ' t.', ' u.', ' s.', ' f.', ' s.', ' ..', ' p.', ' ..', ' a.', ' d.', ' k.', ' r.', ' c.', ' o.', ' r.', ' p.', ' b.', ' z.', ' g.', ' l.', ' b.', ' s.', ' e.', ' n.', ' o.', ' n.', ' o.', ' s.', ' a.', ' r.', ' t.', ' n.', ' r.', ' p.', ' p.', ' c.', ' a.', ' c.', ' a.', ' 1.', ' 2.', ' 3.', ' 4.', ' 5.', ' 6.', ' 7.', ' 8.', ' 9.', ' 1.', ' 0.', ' 1.', ' 1.', ' 1.', ' 2.', ' 1.', ' 3.', ' 1.', ' 4.', ' 1.', ' 5.', ' 1.', ' 6.', ' 1.', ' 7.', ' 1.', ' 8.', ' 1.', ' 9.', ' 2.', ' 0.', ' 2.', ' 1.', ' 2.', ' 2.', ' 2.', ' 3.', ' 2.', ' 4.', ' 2.', ' 5.', ' 2.', ' 6.', ' 2.', ' 7.', ' 2.', ' 8.', ' 2.', ' 9.', ' 3.', ' 0.', ' 3.', ' 1.', ' 3.', ' 2.', ' 3.', ' 3.', ' 3.', ' 4.', ' 3.', ' 5.', ' 3.', ' 6.', ' 3.', ' 7.', ' 3.', ' 8.', ' 3.', ' 9.', ' 4.', ' 0.', ' 4.', ' 1.', ' 4.', ' 2.', ' 4.', ' 3.', ' 4.', ' 4.', ' 4.', ' 5.', ' 4.', ' 6.', ' 4.', ' 7.', ' 4.', ' 8.', ' 4.', ' 9.', ' 5.', ' 0.', ' 5.', ' 1.', ' 5.', ' 2.', ' 5.', ' 3.', ' 5.', ' 4.', ' 5.', ' 5.', ' 5.', ' 6.', ' 5.', ' 7.', ' 5.', ' 8.', ' 5.', ' 9.', ' 6.', ' 0.', ' 6.', ' 1.', ' 6.', ' 2.', ' 6.', ' 3.', ' 6.', ' 4.', ' 6.', ' 5.', ' 6.', ' 6.', ' 6.', ' 7.', ' 6.', ' 8.', ' 6.', ' 9.', ' 7.', ' 0.', ' 7.', ' 1.', ' 7.', ' 2.', ' 7.', ' 3.', ' 7.', ' 4.', ' 7.', ' 5.', ' 7.', ' 6.', ' 7.', ' 7.', ' 7.', ' 8.', ' 7.', ' 9.', ' 8.', ' 0.', ' 8.', ' 1.', ' 8.', ' 2.', ' 8.', ' 3.', ' 8.', ' 4.', ' 8.', ' 5.', ' 8.', ' 6.', ' 8.', ' 7.', ' 8.', ' 8.', ' 8.', ' 9.', ' 9.', ' 0.', ' 9.', ' 1.', ' 9.', ' 2.', ' 9.', ' 3.', ' 9.', ' 4.', ' 9.', ' 5.', ' 9.', ' 6.', ' 9.', ' 7.', ' 9.', ' 8.', ' 9.', ' 9.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for prefix in non_breaking_prefix_en:\n",
        "    text_en = text_en.replace(prefix, prefix + '###')\n",
        "text_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_en)\n",
        "text_en = re.sub(r\"\\.###\", '', text_en)\n",
        "text_en = re.sub(r\" +\", ' ', text_en)\n",
        "text_en = text_en.replace('###', ' ')\n",
        "\n",
        "text_en = text_en.split(\"\\n\")\n",
        "\n",
        "for prefix in non_breaking_prefix_de:\n",
        "    text_de = text_de.replace(prefix, prefix + '###')\n",
        "text_de = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_de)\n",
        "text_de = re.sub(r\"\\.###\", '', text_de)\n",
        "text_de = re.sub(r\" +\", ' ', text_de)\n",
        "text_de = text_de.replace('###', ' ')\n",
        "\n",
        "text_de = text_de.split(\"\\n\")"
      ],
      "metadata": {
        "id": "NeF-h9tU4jaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4n40xDnp0qU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_de)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "McZGOiM_0svC",
        "outputId": "a6de7148-feae-44f5-8455-7035b9a54038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bc00cb8c7e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_de\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text_de' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing**"
      ],
      "metadata": {
        "id": "lxPDTYhs4nch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_en, target_vocab_size=8000)\n",
        "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_de, target_vocab_size=8000)"
      ],
      "metadata": {
        "id": "_HrMZkBX4paf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_DE = tokenizer_de.vocab_size + 2\n",
        "\n",
        "# start and end tokens as size-1 and size-2 which are the same as\n",
        "# tokenizer_size and tokenizer_size+1 because the words are from [0 to ts-1]\n",
        "# tokenize_en.encode(sentence) give a list then list + list + list appends them\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in text_en]\n",
        "outputs = [[VOCAB_SIZE_DE-2] + tokenizer_de.encode(sentence) + [VOCAB_SIZE_DE-1]\n",
        "          for sentence in text_de]"
      ],
      "metadata": {
        "id": "JAj49Buq4tEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Max length for sentence**"
      ],
      "metadata": {
        "id": "eyq1L-E74vwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20 # we will still have a lot of data with max len of 20\n",
        "\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "# removed in reversed because of shifting issuies when start from begining\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# same stuff for outputs>20\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "metadata": {
        "id": "tfz95QJ34vGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**input/output creation**"
      ],
      "metadata": {
        "id": "XpuvNe2Q46a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "iQ8Hckv-473C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**padding\n",
        "batching**"
      ],
      "metadata": {
        "id": "wPqPumqt4-HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 # how much data to keep\n",
        "\n",
        "# turned data into a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "# this is something that improves the way the dataset is stored, it increases\n",
        "# the speed of accessing the data which increases training speed in return:\n",
        "dataset = dataset.cache()\n",
        "\n",
        "# shuffle in batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# this increases the speed even further:\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vg05oUA34-Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "jqHvJzej5HEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**"
      ],
      "metadata": {
        "id": "yuRrELO75LyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # this Positional Encoder is child of the Layers so it has all\n",
        "        # the properties that a layer has\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        \"\"\"\n",
        "        :pos: (seq_len, 1) index of the word in sentence [0 to 19]\n",
        "        :i: the dimensions of the embedding (glove dims 200) then-> [0 to 199]\n",
        "        :d_model: the size (dimension) of the embeded (e.g. glove size 200)\n",
        "        :return: (seq_len, d_model)  getting the encoding of the\n",
        "                every positions vs every one of the dimensions of that word\n",
        "        \"\"\"\n",
        "        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # dim: (seq_len, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input.shape = [batch_size, multihead_size(sz=8), each word (pos), that words embedding]\n",
        "       \n",
        "        seq_length = inputs.shape.as_list()[-2] # basically the pos\n",
        "        d_model = inputs.shape.as_list()[-1] # basically the embedded values\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # it has a [batch] dimension at the begining \n",
        "        # because inputs and the encodings need to be same dims so we\n",
        "        # make newaxis which it doesnt put 0's.... it copies those same dims for\n",
        "        # all the batches...\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        # return both the inputs and their pos_encodings\n",
        "        #  pos_encoding is in np so make them tf\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "metadata": {
        "id": "hwTvQYAJ5JLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention**"
      ],
      "metadata": {
        "id": "-NUNgqvj5R0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Q*K will be [output_len, d_model] * [d_model, input_len] which both are 20\n",
        "    # for both english and german\n",
        "    # the transpose_b=True makes keys turn to keys.T\n",
        "    # each of them are this dim: [batch_size, nb_proj, seq_len, d_proj]\n",
        "    # so with transpose it become: [a,b,c,d] * []\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32) # makes the dim_num float\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim) # scales it (formula stuff)\n",
        "\n",
        "    # because this mask as the paper said is optional to prevent the program\n",
        "    # from seeing the feauture. because when we backprop then they will\n",
        "    # consider the stuff in front of them so to stop this we add -1e9 to them\n",
        "    # so after softmax the probabilities become 0 for them\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    # apply the softmax along the last axis because their sum has to be 1\n",
        "    # scaled_product = [output_len, input_len] -> softmax on input_len so\n",
        "    # basically we are keeping in_len the same but finding the probs for out_len\n",
        "    # so for every ins what is the prob of each of the outs\n",
        "    # (e.g. ith input, the probs [0.3,0.7] of the outs)\n",
        "    probs = tf.nn.softmax(scaled_product, axis=-1)\n",
        "\n",
        "    # attention = [output_len, input_len] * [input_len, d_model] = [output_len, d_model]\n",
        "    # so d_model weights for each of the output words which we will\n",
        "    # feed to forwards to see their prediction for each of the out_lens\n",
        "    attention = tf.matmul(probs, values)\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "1BcRLq3r5SHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Head attention sublayer**"
      ],
      "metadata": {
        "id": "1BPUByjS5R53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, nb_proj):\n",
        "        \"\"\"\n",
        "        :nb_proj: the number of projections for the multihead\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "    \n",
        "    # this is the same as init but it happens when the object for the\n",
        "    # first time is used, in init it was called when we CREATED the object\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # make sure they are divisible\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        #2 slashes to make it integer\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(self.d_model)\n",
        "        self.key_lin = layers.Dense(self.d_model)\n",
        "        self.value_lin = layers.Dense(self.d_model)\n",
        "        self.final_lin = layers.Dense(self.d_model)\n",
        "    \n",
        "    def split_proj(self, inputs, batch_size):\n",
        "        \"\"\"\n",
        "        :inputs: [batch_size, seq_len(20), d_model(prev layer dim)]\n",
        "\n",
        "        :return: \n",
        "            dims = [batch_size, nb_proj, seq_len, d_proj]\n",
        "            nb_proj here is like channels in cnn\n",
        "            \n",
        "        \"\"\"\n",
        "        new_shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        # here we will get: [batch_sz, seq_len, nb_proj, d_proj]\n",
        "\n",
        "        splited_inputs = tf.reshape(inputs, shape=new_shape)\n",
        "\n",
        "        # so we need to reshape it to: [batch_size, nb_proj, seq_len, d_proj]\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "\n",
        "        # now we split each of them to make projs\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        # each of the q,k,v are [batch_size, nb_proj, seq_len, d_proj]\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "        # now we will reverse the splits we did above: reshape + concat\n",
        "        attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "        # we have [batch_size, seq_len, nb_proj, d_proj] so now we concat 2, 3\n",
        "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "SbauNsBQ5SRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "W6Rh2f835gb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        \"\"\"\n",
        "        :FFN_units:\n",
        "            feed forward networks units: the number of units for the\n",
        "            feed forward which you can see in the encoder part of the\n",
        "            paper (right after the attention there is a feed forward...)\n",
        "        :nb_project: \n",
        "            the number of projections we have (8)\n",
        "        :dropout:\n",
        "            the dropout rate e.g. 0.3\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "    \n",
        "    # when we create the Encoder, so no we can get them when we use the\n",
        "    # function using 'build' instead\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we first build the object for the multi-head-attention\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model, activation=\"relu\")\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        \"\"\"\n",
        "        :mask: which we will apply in the multi-head attention\n",
        "        :training: \n",
        "            it is true/false which we use dropout while we train=true to stop\n",
        "            the model from overfiting but we dont use it when we are just\n",
        "            testing (aka. train=false)\n",
        "        \"\"\"\n",
        "   \n",
        "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "\n",
        "        # dropout + normalization after the attention\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        # we do + inputs here because in the architecture they still concat the\n",
        "        # previous inputs to our resulted attention then we normalize it\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        # now we do the dense in our FFN:\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Rr1EIIFf5gle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_encoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        # we put name=name here because the name is something that belongs\n",
        "        # to the layers class, so we tell it to use name=\"encoder\"\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_encoding_layers = nb_encoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(self.nb_encoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        # as per paper's architecture while doing these\n",
        "        # embedding with maybe glove weights....\n",
        "        outputs = self.embedding(inputs)\n",
        "        # paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        # now we do dropout before all the encoding layers\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_encoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "        \n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "5G6isybZ5o0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**"
      ],
      "metadata": {
        "id": "FsyVZae855jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # MHA 1\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "        # MHA 2\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # FFN\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "       \n",
        "\n",
        "        # this is the 1# attention\n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        # training=bool -> so dont do dropout when training=false\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # this is the 2# attention, this is ALOT different than before one\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                enc_outputs,\n",
        "                                                enc_outputs,\n",
        "                                                mask_2)\n",
        "        #  training=bool -> so dont do dropout when training=false\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + inputs)\n",
        "\n",
        "        # the denses\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "3PGmA9yW5-fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_decoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.nb_decoding_layers = nb_decoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(nb_decoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        \n",
        "        outputs = self.embedding(inputs)\n",
        "        #in paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        #  dropout before all the encoding layers\n",
        "        #  training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_decoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                             enc_outputs,\n",
        "                                             mask_1,\n",
        "                                             mask_2,\n",
        "                                             training)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6nIhBdk65_Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer**"
      ],
      "metadata": {
        "id": "_4RG4hzW6ERg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "\n",
        "        # initing the Objects\n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "      \n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "        \n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "     \n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        \n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        # sample of what it produces is in the cell below\n",
        "        #  when we predict the ith word we dont see words from\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        # combining the encoder and decoder and masks here\n",
        "\n",
        "        # creating the mask for encoder\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        # creating the mask for decoder\n",
        "\n",
        "        # mask #1 is for the first decoder attention which uses the\n",
        "        # output, output, output as q/k/v so we get max of the 2 masks for it\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
        "                                self.create_look_ahead_mask(dec_inputs))\n",
        "        # mask #2 is for the second decoder attention in which we use the\n",
        "        # output of the encoder as v/k so we need to do masking on the input\n",
        "        # so that later when doing q*k then *v we can get a correct output\n",
        "        # this is what the video said, but i belive making this None is alot                 \n",
        "        # more correct since we dont actually use the inputs and outputs but\n",
        "        # their already masked and processed outputs from previous attentions\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "kUfcL1pm6GRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "HZ-mXP5x6TFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**"
      ],
      "metadata": {
        "id": "Kgiaj8p96Xxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters:\n",
        "EPOCHS = 20\n",
        "D_MODEL = 128\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_DE,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)"
      ],
      "metadata": {
        "id": "ERdlgbMX6VYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss**"
      ],
      "metadata": {
        "id": "VpR_WKx-6c3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "\n",
        "# 1) example of SparseCategoricalCrossentropy:\n",
        "# y_true = [1, 2]\n",
        "# y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
        "# 2) we made fromlogits=true. it gives out numbers without softmax\n",
        "# applied to them \n",
        "# 3) we made reduction='none' to get\n",
        "# rid of the padded losses ourselves then sum it up...\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "def loss_function(target, pred):\n",
        "    # so by using this mask we will get rid of all the losses that \n",
        "    # corrispond to 0 in our target (aka. y_true)\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0)) # [326, 4, 0] -> [1, 1, 0]\n",
        "    loss_ = loss_object(target, pred) # we got the loss numbers (not their softmax probabilities)\n",
        "    \n",
        "    # make sure that both loss_ and mask have the same data type so we can mult them\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    # we make the computed losses for 0's 0\n",
        "    loss_ *= mask\n",
        "    \n",
        "    # compute the mean loss and return\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# keeps track of losses during training\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "# keeps track of accs during training\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "metadata": {
        "id": "8ScBhz5P6c_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer**"
      ],
      "metadata": {
        "id": "Ox7HdnY36fOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    \n",
        "    # from the tf.keras.optimizers.schedules.LearningRateSchedule \n",
        "    def __call__(self, step):\n",
        "   \n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "_ck-SJ6J6fUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoints**"
      ],
      "metadata": {
        "id": "SUftiDbF6l4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a checkpoint:\n",
        "checkpoint_path = \"./ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\"\"\"\n",
        "# lets check if we already have a checkpoint\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint Restored...\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uASIQ43C6l_9",
        "outputId": "cf67c92c-1acd-4c9b-813d-7b99d707dd82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# lets check if we already have a checkpoint\\nif ckpt_manager.latest_checkpoint:\\n    ckpt.restore(ckpt_manager.latest_checkpoint)\\n    print(\"Latest Checkpoint Restored...\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Epochs**"
      ],
      "metadata": {
        "id": "IhTZ0imf6tK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # iterate on each batch:\n",
        "    for (batch_index, (enc_inputs, targets)) in enumerate(dataset):\n",
        "       \n",
        "        dec_inputs = targets[:, :-1]\n",
        "        \n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "\n",
        "        # this will record everything that happens when we do predictions\n",
        "        with tf.GradientTape() as tape:\n",
        "            # the true is for training\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        #  get the gradients using this method using the tape\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        # apply the gradients according to our Adam optimizer\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        #  print loss and acc from time to time.....\n",
        "        if batch_index % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch_index, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    \n",
        "    # at the end of each epoch save a checkpoint\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}!\".format(epoch+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bQCMOZX60uH",
        "outputId": "2fe8ce0d-7924-4618-d3e1-76764d41fa24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.7986 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 5.9751 Accuracy 0.0060\n",
            "Epoch 1 Batch 100 Loss 5.9192 Accuracy 0.0290\n",
            "Epoch 1 Batch 150 Loss 5.8474 Accuracy 0.0368\n",
            "Epoch 1 Batch 200 Loss 5.7833 Accuracy 0.0407\n",
            "Epoch 1 Batch 250 Loss 5.6946 Accuracy 0.0446\n",
            "Epoch 1 Batch 300 Loss 5.5928 Accuracy 0.0508\n",
            "Epoch 1 Batch 350 Loss 5.4928 Accuracy 0.0565\n",
            "Epoch 1 Batch 400 Loss 5.3875 Accuracy 0.0610\n",
            "Epoch 1 Batch 450 Loss 5.2950 Accuracy 0.0649\n",
            "Epoch 1 Batch 500 Loss 5.1996 Accuracy 0.0692\n",
            "Epoch 1 Batch 550 Loss 5.1146 Accuracy 0.0738\n",
            "Epoch 1 Batch 600 Loss 5.0295 Accuracy 0.0780\n",
            "Epoch 1 Batch 650 Loss 4.9469 Accuracy 0.0821\n",
            "Epoch 1 Batch 700 Loss 4.8704 Accuracy 0.0861\n",
            "Epoch 1 Batch 750 Loss 4.7970 Accuracy 0.0903\n",
            "Epoch 1 Batch 800 Loss 4.7290 Accuracy 0.0944\n",
            "Epoch 1 Batch 850 Loss 4.6604 Accuracy 0.0985\n",
            "Epoch 1 Batch 900 Loss 4.5989 Accuracy 0.1025\n",
            "Epoch 1 Batch 950 Loss 4.5429 Accuracy 0.1063\n",
            "Epoch 1 Batch 1000 Loss 4.4888 Accuracy 0.1102\n",
            "Epoch 1 Batch 1050 Loss 4.4354 Accuracy 0.1137\n",
            "Epoch 1 Batch 1100 Loss 4.3865 Accuracy 0.1171\n",
            "Epoch 1 Batch 1150 Loss 4.3371 Accuracy 0.1202\n",
            "Epoch 1 Batch 1200 Loss 4.2919 Accuracy 0.1231\n",
            "Epoch 1 Batch 1250 Loss 4.2479 Accuracy 0.1258\n",
            "Epoch 1 Batch 1300 Loss 4.2076 Accuracy 0.1284\n",
            "Epoch 1 Batch 1350 Loss 4.1685 Accuracy 0.1309\n",
            "Epoch 1 Batch 1400 Loss 4.1299 Accuracy 0.1334\n",
            "Epoch 1 Batch 1450 Loss 4.0940 Accuracy 0.1358\n",
            "Epoch 1 Batch 1500 Loss 4.0591 Accuracy 0.1381\n",
            "Epoch 1 Batch 1550 Loss 4.0245 Accuracy 0.1404\n",
            "Epoch 1 Batch 1600 Loss 3.9942 Accuracy 0.1425\n",
            "Epoch 1 Batch 1650 Loss 3.9647 Accuracy 0.1446\n",
            "Epoch 1 Batch 1700 Loss 3.9354 Accuracy 0.1466\n",
            "Epoch 1 Batch 1750 Loss 3.9062 Accuracy 0.1485\n",
            "Epoch 1 Batch 1800 Loss 3.8767 Accuracy 0.1504\n",
            "Epoch 1 Batch 1850 Loss 3.8494 Accuracy 0.1523\n",
            "Epoch 1 Batch 1900 Loss 3.8228 Accuracy 0.1541\n",
            "Epoch 1 Batch 1950 Loss 3.7950 Accuracy 0.1558\n",
            "Epoch 1 Batch 2000 Loss 3.7689 Accuracy 0.1575\n",
            "Epoch 1 Batch 2050 Loss 3.7433 Accuracy 0.1591\n",
            "Epoch 1 Batch 2100 Loss 3.7187 Accuracy 0.1607\n",
            "Epoch 1 Batch 2150 Loss 3.6938 Accuracy 0.1623\n",
            "Epoch 1 Batch 2200 Loss 3.6703 Accuracy 0.1638\n",
            "Epoch 1 Batch 2250 Loss 3.6485 Accuracy 0.1653\n",
            "Epoch 1 Batch 2300 Loss 3.6266 Accuracy 0.1667\n",
            "Epoch 1 Batch 2350 Loss 3.6050 Accuracy 0.1681\n",
            "Epoch 1 Batch 2400 Loss 3.5852 Accuracy 0.1696\n",
            "Epoch 1 Batch 2450 Loss 3.5637 Accuracy 0.1711\n",
            "Epoch 1 Batch 2500 Loss 3.5432 Accuracy 0.1726\n",
            "Epoch 1 Batch 2550 Loss 3.5233 Accuracy 0.1740\n",
            "Epoch 1 Batch 2600 Loss 3.5040 Accuracy 0.1755\n",
            "Epoch 1 Batch 2650 Loss 3.4850 Accuracy 0.1770\n",
            "Epoch 1 Batch 2700 Loss 3.4665 Accuracy 0.1786\n",
            "Epoch 1 Batch 2750 Loss 3.4481 Accuracy 0.1800\n",
            "Epoch 1 Batch 2800 Loss 3.4309 Accuracy 0.1814\n",
            "Epoch 1 Batch 2850 Loss 3.4139 Accuracy 0.1828\n",
            "Epoch 1 Batch 2900 Loss 3.3968 Accuracy 0.1842\n",
            "Epoch 1 Batch 2950 Loss 3.3806 Accuracy 0.1856\n",
            "Epoch 1 Batch 3000 Loss 3.3655 Accuracy 0.1870\n",
            "Epoch 1 Batch 3050 Loss 3.3510 Accuracy 0.1884\n",
            "Epoch 1 Batch 3100 Loss 3.3365 Accuracy 0.1897\n",
            "Epoch 1 Batch 3150 Loss 3.3212 Accuracy 0.1910\n",
            "Epoch 1 Batch 3200 Loss 3.3073 Accuracy 0.1924\n",
            "Epoch 1 Batch 3250 Loss 3.2933 Accuracy 0.1937\n",
            "Epoch 1 Batch 3300 Loss 3.2799 Accuracy 0.1950\n",
            "Epoch 1 Batch 3350 Loss 3.2667 Accuracy 0.1963\n",
            "Epoch 1 Batch 3400 Loss 3.2543 Accuracy 0.1975\n",
            "Epoch 1 Batch 3450 Loss 3.2418 Accuracy 0.1986\n",
            "Epoch 1 Batch 3500 Loss 3.2300 Accuracy 0.1998\n",
            "Epoch 1 Batch 3550 Loss 3.2176 Accuracy 0.2010\n",
            "Epoch 1 Batch 3600 Loss 3.2061 Accuracy 0.2021\n",
            "Epoch 1 Batch 3650 Loss 3.1941 Accuracy 0.2032\n",
            "Epoch 1 Batch 3700 Loss 3.1826 Accuracy 0.2043\n",
            "Epoch 1 Batch 3750 Loss 3.1713 Accuracy 0.2054\n",
            "Epoch 1 Batch 3800 Loss 3.1607 Accuracy 0.2065\n",
            "Epoch 1 Batch 3850 Loss 3.1496 Accuracy 0.2076\n",
            "Epoch 1 Batch 3900 Loss 3.1385 Accuracy 0.2087\n",
            "Epoch 1 Batch 3950 Loss 3.1282 Accuracy 0.2097\n",
            "Epoch 1 Batch 4000 Loss 3.1181 Accuracy 0.2107\n",
            "Epoch 1 Batch 4050 Loss 3.1080 Accuracy 0.2117\n",
            "Epoch 1 Batch 4100 Loss 3.0983 Accuracy 0.2128\n",
            "Epoch 1 Batch 4150 Loss 3.0886 Accuracy 0.2138\n",
            "Epoch 1 Batch 4200 Loss 3.0785 Accuracy 0.2148\n",
            "Epoch 1 Batch 4250 Loss 3.0687 Accuracy 0.2158\n",
            "Epoch 1 Batch 4300 Loss 3.0593 Accuracy 0.2168\n",
            "Epoch 1 Batch 4350 Loss 3.0504 Accuracy 0.2178\n",
            "Epoch 1 Batch 4400 Loss 3.0415 Accuracy 0.2188\n",
            "Epoch 1 Batch 4450 Loss 3.0333 Accuracy 0.2196\n",
            "Epoch 1 Batch 4500 Loss 3.0253 Accuracy 0.2204\n",
            "Epoch 1 Batch 4550 Loss 3.0178 Accuracy 0.2212\n",
            "Epoch 1 Batch 4600 Loss 3.0102 Accuracy 0.2219\n",
            "Epoch 1 Batch 4650 Loss 3.0027 Accuracy 0.2225\n",
            "Epoch 1 Batch 4700 Loss 2.9951 Accuracy 0.2232\n",
            "Epoch 1 Batch 4750 Loss 2.9876 Accuracy 0.2238\n",
            "Epoch 1 Batch 4800 Loss 2.9801 Accuracy 0.2244\n",
            "Epoch 1 Batch 4850 Loss 2.9734 Accuracy 0.2249\n",
            "Epoch 1 Batch 4900 Loss 2.9659 Accuracy 0.2256\n",
            "Epoch 1 Batch 4950 Loss 2.9587 Accuracy 0.2262\n",
            "Epoch 1 Batch 5000 Loss 2.9520 Accuracy 0.2268\n",
            "Epoch 1 Batch 5050 Loss 2.9454 Accuracy 0.2274\n",
            "Epoch 1 Batch 5100 Loss 2.9385 Accuracy 0.2279\n",
            "Epoch 1 Batch 5150 Loss 2.9320 Accuracy 0.2286\n",
            "Epoch 1 Batch 5200 Loss 2.9253 Accuracy 0.2291\n",
            "Epoch 1 Batch 5250 Loss 2.9187 Accuracy 0.2297\n",
            "Epoch 1 Batch 5300 Loss 2.9123 Accuracy 0.2303\n",
            "Epoch 1 Batch 5350 Loss 2.9057 Accuracy 0.2309\n",
            "Epoch 1 Batch 5400 Loss 2.8994 Accuracy 0.2314\n",
            "Epoch 1 Batch 5450 Loss 2.8934 Accuracy 0.2319\n",
            "Epoch 1 Batch 5500 Loss 2.8872 Accuracy 0.2324\n",
            "Epoch 1 Batch 5550 Loss 2.8812 Accuracy 0.2329\n",
            "Epoch 1 Batch 5600 Loss 2.8750 Accuracy 0.2334\n",
            "Epoch 1 Batch 5650 Loss 2.8686 Accuracy 0.2339\n",
            "Epoch 1 Batch 5700 Loss 2.8622 Accuracy 0.2343\n",
            "Epoch 1 Batch 5750 Loss 2.8557 Accuracy 0.2348\n",
            "Epoch 1 Batch 5800 Loss 2.8492 Accuracy 0.2352\n",
            "Epoch 1 Batch 5850 Loss 2.8430 Accuracy 0.2357\n",
            "Epoch 1 Batch 5900 Loss 2.8371 Accuracy 0.2362\n",
            "Epoch 1 Batch 5950 Loss 2.8307 Accuracy 0.2367\n",
            "Epoch 1 Batch 6000 Loss 2.8247 Accuracy 0.2371\n",
            "Epoch 1 Batch 6050 Loss 2.8187 Accuracy 0.2376\n",
            "Epoch 1 Batch 6100 Loss 2.8131 Accuracy 0.2380\n",
            "Saved checkpoint for epoch 1!\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 2.3176 Accuracy 0.3010\n",
            "Epoch 2 Batch 50 Loss 2.1491 Accuracy 0.2901\n",
            "Epoch 2 Batch 100 Loss 2.1574 Accuracy 0.2899\n",
            "Epoch 2 Batch 150 Loss 2.1517 Accuracy 0.2909\n",
            "Epoch 2 Batch 200 Loss 2.1513 Accuracy 0.2921\n",
            "Epoch 2 Batch 250 Loss 2.1433 Accuracy 0.2927\n",
            "Epoch 2 Batch 300 Loss 2.1405 Accuracy 0.2934\n",
            "Epoch 2 Batch 350 Loss 2.1363 Accuracy 0.2937\n",
            "Epoch 2 Batch 400 Loss 2.1289 Accuracy 0.2942\n",
            "Epoch 2 Batch 450 Loss 2.1242 Accuracy 0.2947\n",
            "Epoch 2 Batch 500 Loss 2.1221 Accuracy 0.2952\n",
            "Epoch 2 Batch 550 Loss 2.1191 Accuracy 0.2955\n",
            "Epoch 2 Batch 600 Loss 2.1166 Accuracy 0.2957\n",
            "Epoch 2 Batch 650 Loss 2.1161 Accuracy 0.2959\n",
            "Epoch 2 Batch 700 Loss 2.1152 Accuracy 0.2961\n",
            "Epoch 2 Batch 750 Loss 2.1127 Accuracy 0.2962\n",
            "Epoch 2 Batch 800 Loss 2.1107 Accuracy 0.2963\n",
            "Epoch 2 Batch 850 Loss 2.1105 Accuracy 0.2963\n",
            "Epoch 2 Batch 900 Loss 2.1088 Accuracy 0.2963\n",
            "Epoch 2 Batch 950 Loss 2.1076 Accuracy 0.2964\n",
            "Epoch 2 Batch 1000 Loss 2.1082 Accuracy 0.2963\n",
            "Epoch 2 Batch 1050 Loss 2.1065 Accuracy 0.2961\n",
            "Epoch 2 Batch 1100 Loss 2.1045 Accuracy 0.2963\n",
            "Epoch 2 Batch 1150 Loss 2.1016 Accuracy 0.2964\n",
            "Epoch 2 Batch 1200 Loss 2.0992 Accuracy 0.2963\n",
            "Epoch 2 Batch 1250 Loss 2.0967 Accuracy 0.2963\n",
            "Epoch 2 Batch 1300 Loss 2.0964 Accuracy 0.2963\n",
            "Epoch 2 Batch 1350 Loss 2.0949 Accuracy 0.2962\n",
            "Epoch 2 Batch 1400 Loss 2.0932 Accuracy 0.2964\n",
            "Epoch 2 Batch 1450 Loss 2.0921 Accuracy 0.2965\n",
            "Epoch 2 Batch 1500 Loss 2.0904 Accuracy 0.2965\n",
            "Epoch 2 Batch 1550 Loss 2.0881 Accuracy 0.2965\n",
            "Epoch 2 Batch 1600 Loss 2.0858 Accuracy 0.2966\n",
            "Epoch 2 Batch 1650 Loss 2.0843 Accuracy 0.2968\n",
            "Epoch 2 Batch 1700 Loss 2.0818 Accuracy 0.2969\n",
            "Epoch 2 Batch 1750 Loss 2.0803 Accuracy 0.2969\n",
            "Epoch 2 Batch 1800 Loss 2.0782 Accuracy 0.2968\n",
            "Epoch 2 Batch 1850 Loss 2.0756 Accuracy 0.2969\n",
            "Epoch 2 Batch 1900 Loss 2.0729 Accuracy 0.2969\n",
            "Epoch 2 Batch 1950 Loss 2.0691 Accuracy 0.2970\n",
            "Epoch 2 Batch 2000 Loss 2.0672 Accuracy 0.2971\n",
            "Epoch 2 Batch 2050 Loss 2.0643 Accuracy 0.2971\n",
            "Epoch 2 Batch 2100 Loss 2.0616 Accuracy 0.2972\n",
            "Epoch 2 Batch 2150 Loss 2.0587 Accuracy 0.2973\n",
            "Epoch 2 Batch 2200 Loss 2.0563 Accuracy 0.2973\n",
            "Epoch 2 Batch 2250 Loss 2.0539 Accuracy 0.2973\n",
            "Epoch 2 Batch 2300 Loss 2.0514 Accuracy 0.2974\n",
            "Epoch 2 Batch 2350 Loss 2.0486 Accuracy 0.2976\n",
            "Epoch 2 Batch 2400 Loss 2.0449 Accuracy 0.2978\n",
            "Epoch 2 Batch 2450 Loss 2.0416 Accuracy 0.2979\n",
            "Epoch 2 Batch 2500 Loss 2.0380 Accuracy 0.2982\n",
            "Epoch 2 Batch 2550 Loss 2.0346 Accuracy 0.2984\n",
            "Epoch 2 Batch 2600 Loss 2.0315 Accuracy 0.2987\n",
            "Epoch 2 Batch 2650 Loss 2.0288 Accuracy 0.2989\n",
            "Epoch 2 Batch 2700 Loss 2.0263 Accuracy 0.2994\n",
            "Epoch 2 Batch 2750 Loss 2.0235 Accuracy 0.2997\n",
            "Epoch 2 Batch 2800 Loss 2.0203 Accuracy 0.3000\n",
            "Epoch 2 Batch 2850 Loss 2.0181 Accuracy 0.3004\n",
            "Epoch 2 Batch 2900 Loss 2.0160 Accuracy 0.3006\n",
            "Epoch 2 Batch 2950 Loss 2.0146 Accuracy 0.3010\n",
            "Epoch 2 Batch 3000 Loss 2.0124 Accuracy 0.3013\n",
            "Epoch 2 Batch 3050 Loss 2.0102 Accuracy 0.3017\n",
            "Epoch 2 Batch 3100 Loss 2.0086 Accuracy 0.3021\n",
            "Epoch 2 Batch 3150 Loss 2.0065 Accuracy 0.3025\n",
            "Epoch 2 Batch 3200 Loss 2.0050 Accuracy 0.3028\n",
            "Epoch 2 Batch 3250 Loss 2.0037 Accuracy 0.3032\n",
            "Epoch 2 Batch 3300 Loss 2.0030 Accuracy 0.3035\n",
            "Epoch 2 Batch 3350 Loss 2.0014 Accuracy 0.3038\n",
            "Epoch 2 Batch 3400 Loss 1.9997 Accuracy 0.3042\n",
            "Epoch 2 Batch 3450 Loss 1.9982 Accuracy 0.3046\n",
            "Epoch 2 Batch 3500 Loss 1.9966 Accuracy 0.3049\n",
            "Epoch 2 Batch 3550 Loss 1.9950 Accuracy 0.3053\n",
            "Epoch 2 Batch 3600 Loss 1.9936 Accuracy 0.3057\n",
            "Epoch 2 Batch 3650 Loss 1.9920 Accuracy 0.3061\n",
            "Epoch 2 Batch 3700 Loss 1.9906 Accuracy 0.3064\n",
            "Epoch 2 Batch 3750 Loss 1.9893 Accuracy 0.3068\n",
            "Epoch 2 Batch 3800 Loss 1.9882 Accuracy 0.3072\n",
            "Epoch 2 Batch 3850 Loss 1.9864 Accuracy 0.3075\n",
            "Epoch 2 Batch 3900 Loss 1.9849 Accuracy 0.3079\n",
            "Epoch 2 Batch 3950 Loss 1.9834 Accuracy 0.3083\n",
            "Epoch 2 Batch 4000 Loss 1.9819 Accuracy 0.3088\n",
            "Epoch 2 Batch 4050 Loss 1.9804 Accuracy 0.3092\n",
            "Epoch 2 Batch 4100 Loss 1.9789 Accuracy 0.3096\n",
            "Epoch 2 Batch 4150 Loss 1.9774 Accuracy 0.3099\n",
            "Epoch 2 Batch 4200 Loss 1.9757 Accuracy 0.3103\n",
            "Epoch 2 Batch 4250 Loss 1.9748 Accuracy 0.3107\n",
            "Epoch 2 Batch 4300 Loss 1.9733 Accuracy 0.3111\n",
            "Epoch 2 Batch 4350 Loss 1.9715 Accuracy 0.3115\n",
            "Epoch 2 Batch 4400 Loss 1.9700 Accuracy 0.3118\n",
            "Epoch 2 Batch 4450 Loss 1.9692 Accuracy 0.3121\n",
            "Epoch 2 Batch 4500 Loss 1.9691 Accuracy 0.3124\n",
            "Epoch 2 Batch 4550 Loss 1.9690 Accuracy 0.3125\n",
            "Epoch 2 Batch 4600 Loss 1.9686 Accuracy 0.3127\n",
            "Epoch 2 Batch 4650 Loss 1.9683 Accuracy 0.3128\n",
            "Epoch 2 Batch 4700 Loss 1.9682 Accuracy 0.3129\n",
            "Epoch 2 Batch 4750 Loss 1.9679 Accuracy 0.3130\n",
            "Epoch 2 Batch 4800 Loss 1.9681 Accuracy 0.3130\n",
            "Epoch 2 Batch 4850 Loss 1.9680 Accuracy 0.3131\n",
            "Epoch 2 Batch 4900 Loss 1.9680 Accuracy 0.3131\n",
            "Epoch 2 Batch 4950 Loss 1.9680 Accuracy 0.3132\n",
            "Epoch 2 Batch 5000 Loss 1.9677 Accuracy 0.3132\n",
            "Epoch 2 Batch 5050 Loss 1.9674 Accuracy 0.3133\n",
            "Epoch 2 Batch 5100 Loss 1.9672 Accuracy 0.3134\n",
            "Epoch 2 Batch 5150 Loss 1.9668 Accuracy 0.3135\n",
            "Epoch 2 Batch 5200 Loss 1.9669 Accuracy 0.3135\n",
            "Epoch 2 Batch 5250 Loss 1.9668 Accuracy 0.3136\n",
            "Epoch 2 Batch 5300 Loss 1.9662 Accuracy 0.3137\n",
            "Epoch 2 Batch 5350 Loss 1.9659 Accuracy 0.3139\n",
            "Epoch 2 Batch 5400 Loss 1.9655 Accuracy 0.3139\n",
            "Epoch 2 Batch 5450 Loss 1.9650 Accuracy 0.3140\n",
            "Epoch 2 Batch 5500 Loss 1.9641 Accuracy 0.3141\n",
            "Epoch 2 Batch 5550 Loss 1.9634 Accuracy 0.3141\n",
            "Epoch 2 Batch 5600 Loss 1.9629 Accuracy 0.3141\n",
            "Epoch 2 Batch 5650 Loss 1.9623 Accuracy 0.3142\n",
            "Epoch 2 Batch 5700 Loss 1.9614 Accuracy 0.3142\n",
            "Epoch 2 Batch 5750 Loss 1.9609 Accuracy 0.3142\n",
            "Epoch 2 Batch 5800 Loss 1.9601 Accuracy 0.3142\n",
            "Epoch 2 Batch 5850 Loss 1.9593 Accuracy 0.3143\n",
            "Epoch 2 Batch 5900 Loss 1.9586 Accuracy 0.3143\n",
            "Epoch 2 Batch 5950 Loss 1.9579 Accuracy 0.3143\n",
            "Epoch 2 Batch 6000 Loss 1.9568 Accuracy 0.3144\n",
            "Epoch 2 Batch 6050 Loss 1.9562 Accuracy 0.3144\n",
            "Epoch 2 Batch 6100 Loss 1.9552 Accuracy 0.3144\n",
            "Saved checkpoint for epoch 2!\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 2.0252 Accuracy 0.3248\n",
            "Epoch 3 Batch 50 Loss 1.9183 Accuracy 0.3214\n",
            "Epoch 3 Batch 100 Loss 1.9121 Accuracy 0.3212\n",
            "Epoch 3 Batch 150 Loss 1.9001 Accuracy 0.3210\n",
            "Epoch 3 Batch 200 Loss 1.8976 Accuracy 0.3214\n",
            "Epoch 3 Batch 250 Loss 1.9033 Accuracy 0.3211\n",
            "Epoch 3 Batch 300 Loss 1.8949 Accuracy 0.3219\n",
            "Epoch 3 Batch 350 Loss 1.8950 Accuracy 0.3221\n",
            "Epoch 3 Batch 400 Loss 1.8944 Accuracy 0.3217\n",
            "Epoch 3 Batch 450 Loss 1.8911 Accuracy 0.3220\n",
            "Epoch 3 Batch 500 Loss 1.8888 Accuracy 0.3222\n",
            "Epoch 3 Batch 550 Loss 1.8837 Accuracy 0.3222\n",
            "Epoch 3 Batch 600 Loss 1.8832 Accuracy 0.3227\n",
            "Epoch 3 Batch 650 Loss 1.8819 Accuracy 0.3229\n",
            "Epoch 3 Batch 700 Loss 1.8806 Accuracy 0.3228\n",
            "Epoch 3 Batch 750 Loss 1.8785 Accuracy 0.3227\n",
            "Epoch 3 Batch 800 Loss 1.8777 Accuracy 0.3228\n",
            "Epoch 3 Batch 850 Loss 1.8792 Accuracy 0.3227\n",
            "Epoch 3 Batch 900 Loss 1.8800 Accuracy 0.3226\n",
            "Epoch 3 Batch 950 Loss 1.8802 Accuracy 0.3226\n",
            "Epoch 3 Batch 1000 Loss 1.8785 Accuracy 0.3225\n",
            "Epoch 3 Batch 1050 Loss 1.8785 Accuracy 0.3223\n",
            "Epoch 3 Batch 1100 Loss 1.8777 Accuracy 0.3223\n",
            "Epoch 3 Batch 1150 Loss 1.8761 Accuracy 0.3222\n",
            "Epoch 3 Batch 1200 Loss 1.8745 Accuracy 0.3218\n",
            "Epoch 3 Batch 1250 Loss 1.8733 Accuracy 0.3216\n",
            "Epoch 3 Batch 1300 Loss 1.8732 Accuracy 0.3215\n",
            "Epoch 3 Batch 1350 Loss 1.8735 Accuracy 0.3214\n",
            "Epoch 3 Batch 1400 Loss 1.8723 Accuracy 0.3213\n",
            "Epoch 3 Batch 1450 Loss 1.8717 Accuracy 0.3212\n",
            "Epoch 3 Batch 1500 Loss 1.8708 Accuracy 0.3211\n",
            "Epoch 3 Batch 1550 Loss 1.8710 Accuracy 0.3212\n",
            "Epoch 3 Batch 1600 Loss 1.8699 Accuracy 0.3214\n",
            "Epoch 3 Batch 1650 Loss 1.8693 Accuracy 0.3213\n",
            "Epoch 3 Batch 1700 Loss 1.8677 Accuracy 0.3212\n",
            "Epoch 3 Batch 1750 Loss 1.8664 Accuracy 0.3212\n",
            "Epoch 3 Batch 1800 Loss 1.8643 Accuracy 0.3213\n",
            "Epoch 3 Batch 1850 Loss 1.8634 Accuracy 0.3212\n",
            "Epoch 3 Batch 1900 Loss 1.8614 Accuracy 0.3212\n",
            "Epoch 3 Batch 1950 Loss 1.8599 Accuracy 0.3211\n",
            "Epoch 3 Batch 2000 Loss 1.8577 Accuracy 0.3210\n",
            "Epoch 3 Batch 2050 Loss 1.8565 Accuracy 0.3210\n",
            "Epoch 3 Batch 2100 Loss 1.8548 Accuracy 0.3209\n",
            "Epoch 3 Batch 2150 Loss 1.8532 Accuracy 0.3210\n",
            "Epoch 3 Batch 2200 Loss 1.8512 Accuracy 0.3210\n",
            "Epoch 3 Batch 2250 Loss 1.8493 Accuracy 0.3209\n",
            "Epoch 3 Batch 2300 Loss 1.8468 Accuracy 0.3209\n",
            "Epoch 3 Batch 2350 Loss 1.8454 Accuracy 0.3210\n",
            "Epoch 3 Batch 2400 Loss 1.8429 Accuracy 0.3210\n",
            "Epoch 3 Batch 2450 Loss 1.8402 Accuracy 0.3212\n",
            "Epoch 3 Batch 2500 Loss 1.8375 Accuracy 0.3214\n",
            "Epoch 3 Batch 2550 Loss 1.8343 Accuracy 0.3215\n",
            "Epoch 3 Batch 2600 Loss 1.8314 Accuracy 0.3217\n",
            "Epoch 3 Batch 2650 Loss 1.8294 Accuracy 0.3220\n",
            "Epoch 3 Batch 2700 Loss 1.8274 Accuracy 0.3223\n",
            "Epoch 3 Batch 2750 Loss 1.8259 Accuracy 0.3225\n",
            "Epoch 3 Batch 2800 Loss 1.8237 Accuracy 0.3228\n",
            "Epoch 3 Batch 2850 Loss 1.8225 Accuracy 0.3230\n",
            "Epoch 3 Batch 2900 Loss 1.8207 Accuracy 0.3233\n",
            "Epoch 3 Batch 2950 Loss 1.8198 Accuracy 0.3235\n",
            "Epoch 3 Batch 3000 Loss 1.8177 Accuracy 0.3239\n",
            "Epoch 3 Batch 3050 Loss 1.8165 Accuracy 0.3243\n",
            "Epoch 3 Batch 3100 Loss 1.8147 Accuracy 0.3246\n",
            "Epoch 3 Batch 3150 Loss 1.8137 Accuracy 0.3250\n",
            "Epoch 3 Batch 3200 Loss 1.8128 Accuracy 0.3253\n",
            "Epoch 3 Batch 3250 Loss 1.8121 Accuracy 0.3256\n",
            "Epoch 3 Batch 3300 Loss 1.8113 Accuracy 0.3259\n",
            "Epoch 3 Batch 3350 Loss 1.8101 Accuracy 0.3262\n",
            "Epoch 3 Batch 3400 Loss 1.8092 Accuracy 0.3265\n",
            "Epoch 3 Batch 3450 Loss 1.8083 Accuracy 0.3268\n",
            "Epoch 3 Batch 3500 Loss 1.8074 Accuracy 0.3272\n",
            "Epoch 3 Batch 3550 Loss 1.8061 Accuracy 0.3274\n",
            "Epoch 3 Batch 3600 Loss 1.8059 Accuracy 0.3277\n",
            "Epoch 3 Batch 3650 Loss 1.8049 Accuracy 0.3280\n",
            "Epoch 3 Batch 3700 Loss 1.8037 Accuracy 0.3283\n",
            "Epoch 3 Batch 3750 Loss 1.8026 Accuracy 0.3286\n",
            "Epoch 3 Batch 3800 Loss 1.8015 Accuracy 0.3289\n",
            "Epoch 3 Batch 3850 Loss 1.8006 Accuracy 0.3293\n",
            "Epoch 3 Batch 3900 Loss 1.7996 Accuracy 0.3296\n",
            "Epoch 3 Batch 3950 Loss 1.7987 Accuracy 0.3300\n",
            "Epoch 3 Batch 4000 Loss 1.7980 Accuracy 0.3303\n",
            "Epoch 3 Batch 4050 Loss 1.7976 Accuracy 0.3307\n",
            "Epoch 3 Batch 4100 Loss 1.7969 Accuracy 0.3310\n",
            "Epoch 3 Batch 4150 Loss 1.7961 Accuracy 0.3314\n",
            "Epoch 3 Batch 4200 Loss 1.7952 Accuracy 0.3317\n",
            "Epoch 3 Batch 4250 Loss 1.7945 Accuracy 0.3320\n",
            "Epoch 3 Batch 4300 Loss 1.7934 Accuracy 0.3324\n",
            "Epoch 3 Batch 4350 Loss 1.7922 Accuracy 0.3327\n",
            "Epoch 3 Batch 4400 Loss 1.7912 Accuracy 0.3330\n",
            "Epoch 3 Batch 4450 Loss 1.7911 Accuracy 0.3332\n",
            "Epoch 3 Batch 4500 Loss 1.7912 Accuracy 0.3334\n",
            "Epoch 3 Batch 4550 Loss 1.7914 Accuracy 0.3335\n",
            "Epoch 3 Batch 4600 Loss 1.7918 Accuracy 0.3335\n",
            "Epoch 3 Batch 4650 Loss 1.7923 Accuracy 0.3336\n",
            "Epoch 3 Batch 4700 Loss 1.7930 Accuracy 0.3337\n",
            "Epoch 3 Batch 4750 Loss 1.7934 Accuracy 0.3337\n",
            "Epoch 3 Batch 4800 Loss 1.7940 Accuracy 0.3337\n",
            "Epoch 3 Batch 4850 Loss 1.7942 Accuracy 0.3337\n",
            "Epoch 3 Batch 4900 Loss 1.7946 Accuracy 0.3337\n",
            "Epoch 3 Batch 4950 Loss 1.7948 Accuracy 0.3337\n",
            "Epoch 3 Batch 5000 Loss 1.7952 Accuracy 0.3337\n",
            "Epoch 3 Batch 5050 Loss 1.7956 Accuracy 0.3337\n",
            "Epoch 3 Batch 5100 Loss 1.7963 Accuracy 0.3337\n",
            "Epoch 3 Batch 5150 Loss 1.7964 Accuracy 0.3337\n",
            "Epoch 3 Batch 5200 Loss 1.7963 Accuracy 0.3338\n",
            "Epoch 3 Batch 5250 Loss 1.7968 Accuracy 0.3338\n",
            "Epoch 3 Batch 5300 Loss 1.7972 Accuracy 0.3338\n",
            "Epoch 3 Batch 5350 Loss 1.7974 Accuracy 0.3338\n",
            "Epoch 3 Batch 5400 Loss 1.7976 Accuracy 0.3338\n",
            "Epoch 3 Batch 5450 Loss 1.7978 Accuracy 0.3339\n",
            "Epoch 3 Batch 5500 Loss 1.7978 Accuracy 0.3338\n",
            "Epoch 3 Batch 5550 Loss 1.7980 Accuracy 0.3338\n",
            "Epoch 3 Batch 5600 Loss 1.7978 Accuracy 0.3338\n",
            "Epoch 3 Batch 5650 Loss 1.7980 Accuracy 0.3338\n",
            "Epoch 3 Batch 5700 Loss 1.7978 Accuracy 0.3338\n",
            "Epoch 3 Batch 5750 Loss 1.7974 Accuracy 0.3337\n",
            "Epoch 3 Batch 5800 Loss 1.7972 Accuracy 0.3337\n",
            "Epoch 3 Batch 5850 Loss 1.7971 Accuracy 0.3337\n",
            "Epoch 3 Batch 5900 Loss 1.7969 Accuracy 0.3337\n",
            "Epoch 3 Batch 5950 Loss 1.7966 Accuracy 0.3337\n",
            "Epoch 3 Batch 6000 Loss 1.7961 Accuracy 0.3337\n",
            "Epoch 3 Batch 6050 Loss 1.7958 Accuracy 0.3337\n",
            "Epoch 3 Batch 6100 Loss 1.7955 Accuracy 0.3337\n",
            "Saved checkpoint for epoch 3!\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.7826 Accuracy 0.3396\n",
            "Epoch 4 Batch 50 Loss 1.8079 Accuracy 0.3345\n",
            "Epoch 4 Batch 100 Loss 1.8145 Accuracy 0.3333\n",
            "Epoch 4 Batch 150 Loss 1.8025 Accuracy 0.3345\n",
            "Epoch 4 Batch 200 Loss 1.7996 Accuracy 0.3339\n",
            "Epoch 4 Batch 250 Loss 1.7983 Accuracy 0.3340\n",
            "Epoch 4 Batch 300 Loss 1.7957 Accuracy 0.3343\n",
            "Epoch 4 Batch 350 Loss 1.7937 Accuracy 0.3348\n",
            "Epoch 4 Batch 400 Loss 1.7946 Accuracy 0.3349\n",
            "Epoch 4 Batch 450 Loss 1.7953 Accuracy 0.3353\n",
            "Epoch 4 Batch 500 Loss 1.7885 Accuracy 0.3353\n",
            "Epoch 4 Batch 550 Loss 1.7855 Accuracy 0.3357\n",
            "Epoch 4 Batch 600 Loss 1.7853 Accuracy 0.3356\n",
            "Epoch 4 Batch 650 Loss 1.7828 Accuracy 0.3352\n",
            "Epoch 4 Batch 700 Loss 1.7819 Accuracy 0.3353\n",
            "Epoch 4 Batch 750 Loss 1.7813 Accuracy 0.3351\n",
            "Epoch 4 Batch 800 Loss 1.7809 Accuracy 0.3351\n",
            "Epoch 4 Batch 850 Loss 1.7817 Accuracy 0.3353\n",
            "Epoch 4 Batch 900 Loss 1.7823 Accuracy 0.3353\n",
            "Epoch 4 Batch 950 Loss 1.7834 Accuracy 0.3352\n",
            "Epoch 4 Batch 1000 Loss 1.7827 Accuracy 0.3351\n",
            "Epoch 4 Batch 1050 Loss 1.7820 Accuracy 0.3349\n",
            "Epoch 4 Batch 1100 Loss 1.7810 Accuracy 0.3348\n",
            "Epoch 4 Batch 1150 Loss 1.7809 Accuracy 0.3347\n",
            "Epoch 4 Batch 1200 Loss 1.7789 Accuracy 0.3343\n",
            "Epoch 4 Batch 1250 Loss 1.7805 Accuracy 0.3342\n",
            "Epoch 4 Batch 1300 Loss 1.7797 Accuracy 0.3339\n",
            "Epoch 4 Batch 1350 Loss 1.7798 Accuracy 0.3339\n",
            "Epoch 4 Batch 1400 Loss 1.7791 Accuracy 0.3339\n",
            "Epoch 4 Batch 1450 Loss 1.7790 Accuracy 0.3337\n",
            "Epoch 4 Batch 1500 Loss 1.7785 Accuracy 0.3336\n",
            "Epoch 4 Batch 1550 Loss 1.7767 Accuracy 0.3337\n",
            "Epoch 4 Batch 1600 Loss 1.7763 Accuracy 0.3336\n",
            "Epoch 4 Batch 1650 Loss 1.7742 Accuracy 0.3336\n",
            "Epoch 4 Batch 1700 Loss 1.7728 Accuracy 0.3336\n",
            "Epoch 4 Batch 1750 Loss 1.7723 Accuracy 0.3336\n",
            "Epoch 4 Batch 1800 Loss 1.7715 Accuracy 0.3335\n",
            "Epoch 4 Batch 1850 Loss 1.7696 Accuracy 0.3334\n",
            "Epoch 4 Batch 1900 Loss 1.7682 Accuracy 0.3333\n",
            "Epoch 4 Batch 1950 Loss 1.7663 Accuracy 0.3332\n",
            "Epoch 4 Batch 2000 Loss 1.7649 Accuracy 0.3331\n",
            "Epoch 4 Batch 2050 Loss 1.7635 Accuracy 0.3330\n",
            "Epoch 4 Batch 2100 Loss 1.7619 Accuracy 0.3330\n",
            "Epoch 4 Batch 2150 Loss 1.7600 Accuracy 0.3329\n",
            "Epoch 4 Batch 2200 Loss 1.7583 Accuracy 0.3328\n",
            "Epoch 4 Batch 2250 Loss 1.7562 Accuracy 0.3328\n",
            "Epoch 4 Batch 2300 Loss 1.7550 Accuracy 0.3328\n",
            "Epoch 4 Batch 2350 Loss 1.7533 Accuracy 0.3330\n",
            "Epoch 4 Batch 2400 Loss 1.7510 Accuracy 0.3330\n",
            "Epoch 4 Batch 2450 Loss 1.7486 Accuracy 0.3331\n",
            "Epoch 4 Batch 2500 Loss 1.7459 Accuracy 0.3332\n",
            "Epoch 4 Batch 2550 Loss 1.7438 Accuracy 0.3334\n",
            "Epoch 4 Batch 2600 Loss 1.7416 Accuracy 0.3336\n",
            "Epoch 4 Batch 2650 Loss 1.7398 Accuracy 0.3338\n",
            "Epoch 4 Batch 2700 Loss 1.7382 Accuracy 0.3340\n",
            "Epoch 4 Batch 2750 Loss 1.7364 Accuracy 0.3342\n",
            "Epoch 4 Batch 2800 Loss 1.7347 Accuracy 0.3344\n",
            "Epoch 4 Batch 2850 Loss 1.7338 Accuracy 0.3347\n",
            "Epoch 4 Batch 2900 Loss 1.7319 Accuracy 0.3350\n",
            "Epoch 4 Batch 2950 Loss 1.7311 Accuracy 0.3353\n",
            "Epoch 4 Batch 3000 Loss 1.7298 Accuracy 0.3356\n",
            "Epoch 4 Batch 3050 Loss 1.7287 Accuracy 0.3359\n",
            "Epoch 4 Batch 3100 Loss 1.7277 Accuracy 0.3362\n",
            "Epoch 4 Batch 3150 Loss 1.7263 Accuracy 0.3365\n",
            "Epoch 4 Batch 3200 Loss 1.7257 Accuracy 0.3368\n",
            "Epoch 4 Batch 3250 Loss 1.7249 Accuracy 0.3372\n",
            "Epoch 4 Batch 3300 Loss 1.7241 Accuracy 0.3375\n",
            "Epoch 4 Batch 3350 Loss 1.7236 Accuracy 0.3377\n",
            "Epoch 4 Batch 3400 Loss 1.7230 Accuracy 0.3379\n",
            "Epoch 4 Batch 3450 Loss 1.7217 Accuracy 0.3382\n",
            "Epoch 4 Batch 3500 Loss 1.7209 Accuracy 0.3385\n",
            "Epoch 4 Batch 3550 Loss 1.7201 Accuracy 0.3388\n",
            "Epoch 4 Batch 3600 Loss 1.7192 Accuracy 0.3391\n",
            "Epoch 4 Batch 3650 Loss 1.7182 Accuracy 0.3394\n",
            "Epoch 4 Batch 3700 Loss 1.7179 Accuracy 0.3397\n",
            "Epoch 4 Batch 3750 Loss 1.7169 Accuracy 0.3400\n",
            "Epoch 4 Batch 3800 Loss 1.7165 Accuracy 0.3403\n",
            "Epoch 4 Batch 3850 Loss 1.7160 Accuracy 0.3406\n",
            "Epoch 4 Batch 3900 Loss 1.7155 Accuracy 0.3408\n",
            "Epoch 4 Batch 3950 Loss 1.7150 Accuracy 0.3412\n",
            "Epoch 4 Batch 4000 Loss 1.7141 Accuracy 0.3415\n",
            "Epoch 4 Batch 4050 Loss 1.7132 Accuracy 0.3418\n",
            "Epoch 4 Batch 4100 Loss 1.7124 Accuracy 0.3422\n",
            "Epoch 4 Batch 4150 Loss 1.7116 Accuracy 0.3425\n",
            "Epoch 4 Batch 4200 Loss 1.7106 Accuracy 0.3428\n",
            "Epoch 4 Batch 4250 Loss 1.7101 Accuracy 0.3432\n",
            "Epoch 4 Batch 4300 Loss 1.7094 Accuracy 0.3435\n",
            "Epoch 4 Batch 4350 Loss 1.7087 Accuracy 0.3438\n",
            "Epoch 4 Batch 4400 Loss 1.7079 Accuracy 0.3441\n",
            "Epoch 4 Batch 4450 Loss 1.7081 Accuracy 0.3443\n",
            "Epoch 4 Batch 4500 Loss 1.7084 Accuracy 0.3445\n",
            "Epoch 4 Batch 4550 Loss 1.7091 Accuracy 0.3445\n",
            "Epoch 4 Batch 4600 Loss 1.7096 Accuracy 0.3446\n",
            "Epoch 4 Batch 4650 Loss 1.7101 Accuracy 0.3446\n",
            "Epoch 4 Batch 4700 Loss 1.7106 Accuracy 0.3447\n",
            "Epoch 4 Batch 4750 Loss 1.7113 Accuracy 0.3447\n",
            "Epoch 4 Batch 4800 Loss 1.7120 Accuracy 0.3447\n",
            "Epoch 4 Batch 4850 Loss 1.7127 Accuracy 0.3446\n",
            "Epoch 4 Batch 4900 Loss 1.7134 Accuracy 0.3446\n",
            "Epoch 4 Batch 4950 Loss 1.7138 Accuracy 0.3446\n",
            "Epoch 4 Batch 5000 Loss 1.7144 Accuracy 0.3446\n",
            "Epoch 4 Batch 5050 Loss 1.7151 Accuracy 0.3446\n",
            "Epoch 4 Batch 5100 Loss 1.7158 Accuracy 0.3446\n",
            "Epoch 4 Batch 5150 Loss 1.7163 Accuracy 0.3446\n",
            "Epoch 4 Batch 5200 Loss 1.7166 Accuracy 0.3446\n",
            "Epoch 4 Batch 5250 Loss 1.7167 Accuracy 0.3446\n",
            "Epoch 4 Batch 5300 Loss 1.7171 Accuracy 0.3446\n",
            "Epoch 4 Batch 5350 Loss 1.7176 Accuracy 0.3446\n",
            "Epoch 4 Batch 5400 Loss 1.7180 Accuracy 0.3446\n",
            "Epoch 4 Batch 5450 Loss 1.7181 Accuracy 0.3446\n",
            "Epoch 4 Batch 5500 Loss 1.7185 Accuracy 0.3446\n",
            "Epoch 4 Batch 5550 Loss 1.7187 Accuracy 0.3446\n",
            "Epoch 4 Batch 5600 Loss 1.7190 Accuracy 0.3445\n",
            "Epoch 4 Batch 5650 Loss 1.7190 Accuracy 0.3445\n",
            "Epoch 4 Batch 5700 Loss 1.7192 Accuracy 0.3444\n",
            "Epoch 4 Batch 5750 Loss 1.7191 Accuracy 0.3443\n",
            "Epoch 4 Batch 5800 Loss 1.7191 Accuracy 0.3443\n",
            "Epoch 4 Batch 5850 Loss 1.7189 Accuracy 0.3442\n",
            "Epoch 4 Batch 5900 Loss 1.7187 Accuracy 0.3442\n",
            "Epoch 4 Batch 5950 Loss 1.7184 Accuracy 0.3441\n",
            "Epoch 4 Batch 6000 Loss 1.7181 Accuracy 0.3441\n",
            "Epoch 4 Batch 6050 Loss 1.7182 Accuracy 0.3441\n",
            "Epoch 4 Batch 6100 Loss 1.7177 Accuracy 0.3441\n",
            "Saved checkpoint for epoch 4!\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.7949 Accuracy 0.3183\n",
            "Epoch 5 Batch 50 Loss 1.7388 Accuracy 0.3391\n",
            "Epoch 5 Batch 100 Loss 1.7266 Accuracy 0.3396\n",
            "Epoch 5 Batch 150 Loss 1.7342 Accuracy 0.3409\n",
            "Epoch 5 Batch 200 Loss 1.7400 Accuracy 0.3416\n",
            "Epoch 5 Batch 250 Loss 1.7424 Accuracy 0.3423\n",
            "Epoch 5 Batch 300 Loss 1.7449 Accuracy 0.3428\n",
            "Epoch 5 Batch 350 Loss 1.7391 Accuracy 0.3429\n",
            "Epoch 5 Batch 400 Loss 1.7385 Accuracy 0.3430\n",
            "Epoch 5 Batch 450 Loss 1.7374 Accuracy 0.3429\n",
            "Epoch 5 Batch 500 Loss 1.7345 Accuracy 0.3426\n",
            "Epoch 5 Batch 550 Loss 1.7287 Accuracy 0.3432\n",
            "Epoch 5 Batch 600 Loss 1.7283 Accuracy 0.3431\n",
            "Epoch 5 Batch 650 Loss 1.7283 Accuracy 0.3428\n",
            "Epoch 5 Batch 700 Loss 1.7268 Accuracy 0.3427\n",
            "Epoch 5 Batch 750 Loss 1.7268 Accuracy 0.3425\n",
            "Epoch 5 Batch 800 Loss 1.7260 Accuracy 0.3425\n",
            "Epoch 5 Batch 850 Loss 1.7252 Accuracy 0.3423\n",
            "Epoch 5 Batch 900 Loss 1.7265 Accuracy 0.3422\n",
            "Epoch 5 Batch 950 Loss 1.7267 Accuracy 0.3422\n",
            "Epoch 5 Batch 1000 Loss 1.7270 Accuracy 0.3421\n",
            "Epoch 5 Batch 1050 Loss 1.7261 Accuracy 0.3419\n",
            "Epoch 5 Batch 1100 Loss 1.7264 Accuracy 0.3418\n",
            "Epoch 5 Batch 1150 Loss 1.7256 Accuracy 0.3419\n",
            "Epoch 5 Batch 1200 Loss 1.7233 Accuracy 0.3418\n",
            "Epoch 5 Batch 1250 Loss 1.7240 Accuracy 0.3415\n",
            "Epoch 5 Batch 1300 Loss 1.7238 Accuracy 0.3414\n",
            "Epoch 5 Batch 1350 Loss 1.7249 Accuracy 0.3413\n",
            "Epoch 5 Batch 1400 Loss 1.7248 Accuracy 0.3409\n",
            "Epoch 5 Batch 1450 Loss 1.7222 Accuracy 0.3408\n",
            "Epoch 5 Batch 1500 Loss 1.7216 Accuracy 0.3408\n",
            "Epoch 5 Batch 1550 Loss 1.7214 Accuracy 0.3408\n",
            "Epoch 5 Batch 1600 Loss 1.7213 Accuracy 0.3408\n",
            "Epoch 5 Batch 1650 Loss 1.7209 Accuracy 0.3409\n",
            "Epoch 5 Batch 1700 Loss 1.7196 Accuracy 0.3409\n",
            "Epoch 5 Batch 1750 Loss 1.7178 Accuracy 0.3409\n",
            "Epoch 5 Batch 1800 Loss 1.7173 Accuracy 0.3409\n",
            "Epoch 5 Batch 1850 Loss 1.7153 Accuracy 0.3408\n",
            "Epoch 5 Batch 1900 Loss 1.7140 Accuracy 0.3407\n",
            "Epoch 5 Batch 1950 Loss 1.7123 Accuracy 0.3406\n",
            "Epoch 5 Batch 2000 Loss 1.7108 Accuracy 0.3405\n",
            "Epoch 5 Batch 2050 Loss 1.7092 Accuracy 0.3404\n",
            "Epoch 5 Batch 2100 Loss 1.7080 Accuracy 0.3402\n",
            "Epoch 5 Batch 2150 Loss 1.7067 Accuracy 0.3402\n",
            "Epoch 5 Batch 2200 Loss 1.7048 Accuracy 0.3403\n",
            "Epoch 5 Batch 2250 Loss 1.7032 Accuracy 0.3402\n",
            "Epoch 5 Batch 2300 Loss 1.7020 Accuracy 0.3402\n",
            "Epoch 5 Batch 2350 Loss 1.7004 Accuracy 0.3402\n",
            "Epoch 5 Batch 2400 Loss 1.6987 Accuracy 0.3403\n",
            "Epoch 5 Batch 2450 Loss 1.6963 Accuracy 0.3405\n",
            "Epoch 5 Batch 2500 Loss 1.6937 Accuracy 0.3406\n",
            "Epoch 5 Batch 2550 Loss 1.6911 Accuracy 0.3407\n",
            "Epoch 5 Batch 2600 Loss 1.6895 Accuracy 0.3408\n",
            "Epoch 5 Batch 2650 Loss 1.6875 Accuracy 0.3410\n",
            "Epoch 5 Batch 2700 Loss 1.6855 Accuracy 0.3413\n",
            "Epoch 5 Batch 2750 Loss 1.6837 Accuracy 0.3415\n",
            "Epoch 5 Batch 2800 Loss 1.6827 Accuracy 0.3417\n",
            "Epoch 5 Batch 2850 Loss 1.6812 Accuracy 0.3419\n",
            "Epoch 5 Batch 2900 Loss 1.6798 Accuracy 0.3421\n",
            "Epoch 5 Batch 2950 Loss 1.6790 Accuracy 0.3424\n",
            "Epoch 5 Batch 3000 Loss 1.6779 Accuracy 0.3427\n",
            "Epoch 5 Batch 3050 Loss 1.6769 Accuracy 0.3430\n",
            "Epoch 5 Batch 3100 Loss 1.6765 Accuracy 0.3432\n",
            "Epoch 5 Batch 3150 Loss 1.6756 Accuracy 0.3435\n",
            "Epoch 5 Batch 3200 Loss 1.6749 Accuracy 0.3438\n",
            "Epoch 5 Batch 3250 Loss 1.6745 Accuracy 0.3441\n",
            "Epoch 5 Batch 3300 Loss 1.6736 Accuracy 0.3444\n",
            "Epoch 5 Batch 3350 Loss 1.6733 Accuracy 0.3447\n",
            "Epoch 5 Batch 3400 Loss 1.6727 Accuracy 0.3450\n",
            "Epoch 5 Batch 3450 Loss 1.6716 Accuracy 0.3452\n",
            "Epoch 5 Batch 3500 Loss 1.6711 Accuracy 0.3455\n",
            "Epoch 5 Batch 3550 Loss 1.6701 Accuracy 0.3459\n",
            "Epoch 5 Batch 3600 Loss 1.6690 Accuracy 0.3461\n",
            "Epoch 5 Batch 3650 Loss 1.6681 Accuracy 0.3464\n",
            "Epoch 5 Batch 3700 Loss 1.6675 Accuracy 0.3466\n",
            "Epoch 5 Batch 3750 Loss 1.6670 Accuracy 0.3470\n",
            "Epoch 5 Batch 3800 Loss 1.6661 Accuracy 0.3472\n",
            "Epoch 5 Batch 3850 Loss 1.6654 Accuracy 0.3475\n",
            "Epoch 5 Batch 3900 Loss 1.6644 Accuracy 0.3479\n",
            "Epoch 5 Batch 3950 Loss 1.6637 Accuracy 0.3482\n",
            "Epoch 5 Batch 4000 Loss 1.6631 Accuracy 0.3485\n",
            "Epoch 5 Batch 4050 Loss 1.6621 Accuracy 0.3488\n",
            "Epoch 5 Batch 4100 Loss 1.6618 Accuracy 0.3491\n",
            "Epoch 5 Batch 4150 Loss 1.6616 Accuracy 0.3494\n",
            "Epoch 5 Batch 4200 Loss 1.6612 Accuracy 0.3498\n",
            "Epoch 5 Batch 4250 Loss 1.6607 Accuracy 0.3501\n",
            "Epoch 5 Batch 4300 Loss 1.6602 Accuracy 0.3504\n",
            "Epoch 5 Batch 4350 Loss 1.6594 Accuracy 0.3508\n",
            "Epoch 5 Batch 4400 Loss 1.6591 Accuracy 0.3510\n",
            "Epoch 5 Batch 4450 Loss 1.6587 Accuracy 0.3513\n",
            "Epoch 5 Batch 4500 Loss 1.6588 Accuracy 0.3514\n",
            "Epoch 5 Batch 4550 Loss 1.6594 Accuracy 0.3514\n",
            "Epoch 5 Batch 4600 Loss 1.6597 Accuracy 0.3515\n",
            "Epoch 5 Batch 4650 Loss 1.6602 Accuracy 0.3515\n",
            "Epoch 5 Batch 4700 Loss 1.6613 Accuracy 0.3515\n",
            "Epoch 5 Batch 4750 Loss 1.6620 Accuracy 0.3515\n",
            "Epoch 5 Batch 4800 Loss 1.6626 Accuracy 0.3514\n",
            "Epoch 5 Batch 4850 Loss 1.6636 Accuracy 0.3514\n",
            "Epoch 5 Batch 4900 Loss 1.6642 Accuracy 0.3513\n",
            "Epoch 5 Batch 4950 Loss 1.6651 Accuracy 0.3513\n",
            "Epoch 5 Batch 5000 Loss 1.6660 Accuracy 0.3513\n",
            "Epoch 5 Batch 5050 Loss 1.6666 Accuracy 0.3513\n",
            "Epoch 5 Batch 5100 Loss 1.6676 Accuracy 0.3513\n",
            "Epoch 5 Batch 5150 Loss 1.6682 Accuracy 0.3513\n",
            "Epoch 5 Batch 5200 Loss 1.6683 Accuracy 0.3513\n",
            "Epoch 5 Batch 5250 Loss 1.6687 Accuracy 0.3513\n",
            "Epoch 5 Batch 5300 Loss 1.6691 Accuracy 0.3513\n",
            "Epoch 5 Batch 5350 Loss 1.6693 Accuracy 0.3513\n",
            "Epoch 5 Batch 5400 Loss 1.6698 Accuracy 0.3513\n",
            "Epoch 5 Batch 5450 Loss 1.6701 Accuracy 0.3512\n",
            "Epoch 5 Batch 5500 Loss 1.6704 Accuracy 0.3512\n",
            "Epoch 5 Batch 5550 Loss 1.6710 Accuracy 0.3511\n",
            "Epoch 5 Batch 5600 Loss 1.6710 Accuracy 0.3511\n",
            "Epoch 5 Batch 5650 Loss 1.6714 Accuracy 0.3510\n",
            "Epoch 5 Batch 5700 Loss 1.6716 Accuracy 0.3509\n",
            "Epoch 5 Batch 5750 Loss 1.6717 Accuracy 0.3509\n",
            "Epoch 5 Batch 5800 Loss 1.6721 Accuracy 0.3508\n",
            "Epoch 5 Batch 5850 Loss 1.6721 Accuracy 0.3508\n",
            "Epoch 5 Batch 5900 Loss 1.6719 Accuracy 0.3507\n",
            "Epoch 5 Batch 5950 Loss 1.6716 Accuracy 0.3507\n",
            "Epoch 5 Batch 6000 Loss 1.6716 Accuracy 0.3506\n",
            "Epoch 5 Batch 6050 Loss 1.6714 Accuracy 0.3506\n",
            "Epoch 5 Batch 6100 Loss 1.6712 Accuracy 0.3505\n",
            "Saved checkpoint for epoch 5!\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.7228 Accuracy 0.3536\n",
            "Epoch 6 Batch 50 Loss 1.7238 Accuracy 0.3439\n",
            "Epoch 6 Batch 100 Loss 1.7298 Accuracy 0.3466\n",
            "Epoch 6 Batch 150 Loss 1.7156 Accuracy 0.3468\n",
            "Epoch 6 Batch 200 Loss 1.7094 Accuracy 0.3460\n",
            "Epoch 6 Batch 250 Loss 1.7079 Accuracy 0.3461\n",
            "Epoch 6 Batch 300 Loss 1.7018 Accuracy 0.3469\n",
            "Epoch 6 Batch 350 Loss 1.7031 Accuracy 0.3470\n",
            "Epoch 6 Batch 400 Loss 1.7033 Accuracy 0.3468\n",
            "Epoch 6 Batch 450 Loss 1.7027 Accuracy 0.3468\n",
            "Epoch 6 Batch 500 Loss 1.7005 Accuracy 0.3471\n",
            "Epoch 6 Batch 550 Loss 1.6976 Accuracy 0.3474\n",
            "Epoch 6 Batch 600 Loss 1.6978 Accuracy 0.3474\n",
            "Epoch 6 Batch 650 Loss 1.6950 Accuracy 0.3477\n",
            "Epoch 6 Batch 700 Loss 1.6944 Accuracy 0.3476\n",
            "Epoch 6 Batch 750 Loss 1.6921 Accuracy 0.3478\n",
            "Epoch 6 Batch 800 Loss 1.6931 Accuracy 0.3478\n",
            "Epoch 6 Batch 850 Loss 1.6923 Accuracy 0.3477\n",
            "Epoch 6 Batch 900 Loss 1.6932 Accuracy 0.3477\n",
            "Epoch 6 Batch 950 Loss 1.6931 Accuracy 0.3476\n",
            "Epoch 6 Batch 1000 Loss 1.6927 Accuracy 0.3477\n",
            "Epoch 6 Batch 1050 Loss 1.6927 Accuracy 0.3475\n",
            "Epoch 6 Batch 1100 Loss 1.6914 Accuracy 0.3474\n",
            "Epoch 6 Batch 1150 Loss 1.6908 Accuracy 0.3472\n",
            "Epoch 6 Batch 1200 Loss 1.6889 Accuracy 0.3471\n",
            "Epoch 6 Batch 1250 Loss 1.6886 Accuracy 0.3468\n",
            "Epoch 6 Batch 1300 Loss 1.6865 Accuracy 0.3467\n",
            "Epoch 6 Batch 1350 Loss 1.6861 Accuracy 0.3464\n",
            "Epoch 6 Batch 1400 Loss 1.6875 Accuracy 0.3463\n",
            "Epoch 6 Batch 1450 Loss 1.6872 Accuracy 0.3461\n",
            "Epoch 6 Batch 1500 Loss 1.6867 Accuracy 0.3462\n",
            "Epoch 6 Batch 1550 Loss 1.6864 Accuracy 0.3461\n",
            "Epoch 6 Batch 1600 Loss 1.6856 Accuracy 0.3462\n",
            "Epoch 6 Batch 1650 Loss 1.6846 Accuracy 0.3462\n",
            "Epoch 6 Batch 1700 Loss 1.6832 Accuracy 0.3461\n",
            "Epoch 6 Batch 1750 Loss 1.6829 Accuracy 0.3460\n",
            "Epoch 6 Batch 1800 Loss 1.6819 Accuracy 0.3461\n",
            "Epoch 6 Batch 1850 Loss 1.6806 Accuracy 0.3461\n",
            "Epoch 6 Batch 1900 Loss 1.6793 Accuracy 0.3460\n",
            "Epoch 6 Batch 1950 Loss 1.6774 Accuracy 0.3459\n",
            "Epoch 6 Batch 2000 Loss 1.6769 Accuracy 0.3458\n",
            "Epoch 6 Batch 2050 Loss 1.6754 Accuracy 0.3458\n",
            "Epoch 6 Batch 2100 Loss 1.6733 Accuracy 0.3457\n",
            "Epoch 6 Batch 2150 Loss 1.6723 Accuracy 0.3455\n",
            "Epoch 6 Batch 2200 Loss 1.6702 Accuracy 0.3454\n",
            "Epoch 6 Batch 2250 Loss 1.6692 Accuracy 0.3455\n",
            "Epoch 6 Batch 2300 Loss 1.6673 Accuracy 0.3455\n",
            "Epoch 6 Batch 2350 Loss 1.6656 Accuracy 0.3455\n",
            "Epoch 6 Batch 2400 Loss 1.6635 Accuracy 0.3455\n",
            "Epoch 6 Batch 2450 Loss 1.6614 Accuracy 0.3456\n",
            "Epoch 6 Batch 2500 Loss 1.6588 Accuracy 0.3457\n",
            "Epoch 6 Batch 2550 Loss 1.6556 Accuracy 0.3458\n",
            "Epoch 6 Batch 2600 Loss 1.6535 Accuracy 0.3460\n",
            "Epoch 6 Batch 2650 Loss 1.6521 Accuracy 0.3462\n",
            "Epoch 6 Batch 2700 Loss 1.6507 Accuracy 0.3464\n",
            "Epoch 6 Batch 2750 Loss 1.6489 Accuracy 0.3466\n",
            "Epoch 6 Batch 2800 Loss 1.6474 Accuracy 0.3467\n",
            "Epoch 6 Batch 2850 Loss 1.6461 Accuracy 0.3469\n",
            "Epoch 6 Batch 2900 Loss 1.6444 Accuracy 0.3473\n",
            "Epoch 6 Batch 2950 Loss 1.6434 Accuracy 0.3475\n",
            "Epoch 6 Batch 3000 Loss 1.6420 Accuracy 0.3479\n",
            "Epoch 6 Batch 3050 Loss 1.6409 Accuracy 0.3482\n",
            "Epoch 6 Batch 3100 Loss 1.6397 Accuracy 0.3484\n",
            "Epoch 6 Batch 3150 Loss 1.6391 Accuracy 0.3488\n",
            "Epoch 6 Batch 3200 Loss 1.6384 Accuracy 0.3491\n",
            "Epoch 6 Batch 3250 Loss 1.6381 Accuracy 0.3494\n",
            "Epoch 6 Batch 3300 Loss 1.6374 Accuracy 0.3497\n",
            "Epoch 6 Batch 3350 Loss 1.6371 Accuracy 0.3500\n",
            "Epoch 6 Batch 3400 Loss 1.6363 Accuracy 0.3502\n",
            "Epoch 6 Batch 3450 Loss 1.6357 Accuracy 0.3505\n",
            "Epoch 6 Batch 3500 Loss 1.6346 Accuracy 0.3507\n",
            "Epoch 6 Batch 3550 Loss 1.6343 Accuracy 0.3510\n",
            "Epoch 6 Batch 3600 Loss 1.6331 Accuracy 0.3513\n",
            "Epoch 6 Batch 3650 Loss 1.6322 Accuracy 0.3515\n",
            "Epoch 6 Batch 3700 Loss 1.6317 Accuracy 0.3518\n",
            "Epoch 6 Batch 3750 Loss 1.6311 Accuracy 0.3521\n",
            "Epoch 6 Batch 3800 Loss 1.6302 Accuracy 0.3523\n",
            "Epoch 6 Batch 3850 Loss 1.6294 Accuracy 0.3526\n",
            "Epoch 6 Batch 3900 Loss 1.6284 Accuracy 0.3530\n",
            "Epoch 6 Batch 3950 Loss 1.6278 Accuracy 0.3532\n",
            "Epoch 6 Batch 4000 Loss 1.6273 Accuracy 0.3536\n",
            "Epoch 6 Batch 4050 Loss 1.6266 Accuracy 0.3539\n",
            "Epoch 6 Batch 4100 Loss 1.6261 Accuracy 0.3542\n",
            "Epoch 6 Batch 4150 Loss 1.6260 Accuracy 0.3545\n",
            "Epoch 6 Batch 4200 Loss 1.6254 Accuracy 0.3548\n",
            "Epoch 6 Batch 4250 Loss 1.6246 Accuracy 0.3552\n",
            "Epoch 6 Batch 4300 Loss 1.6239 Accuracy 0.3555\n",
            "Epoch 6 Batch 4350 Loss 1.6234 Accuracy 0.3558\n",
            "Epoch 6 Batch 4400 Loss 1.6231 Accuracy 0.3561\n",
            "Epoch 6 Batch 4450 Loss 1.6229 Accuracy 0.3563\n",
            "Epoch 6 Batch 4500 Loss 1.6233 Accuracy 0.3564\n",
            "Epoch 6 Batch 4550 Loss 1.6240 Accuracy 0.3565\n",
            "Epoch 6 Batch 4600 Loss 1.6248 Accuracy 0.3566\n",
            "Epoch 6 Batch 4650 Loss 1.6256 Accuracy 0.3566\n",
            "Epoch 6 Batch 4700 Loss 1.6264 Accuracy 0.3566\n",
            "Epoch 6 Batch 4750 Loss 1.6272 Accuracy 0.3565\n",
            "Epoch 6 Batch 4800 Loss 1.6280 Accuracy 0.3565\n",
            "Epoch 6 Batch 4850 Loss 1.6288 Accuracy 0.3565\n",
            "Epoch 6 Batch 4900 Loss 1.6298 Accuracy 0.3565\n",
            "Epoch 6 Batch 4950 Loss 1.6304 Accuracy 0.3565\n",
            "Epoch 6 Batch 5000 Loss 1.6310 Accuracy 0.3564\n",
            "Epoch 6 Batch 5050 Loss 1.6319 Accuracy 0.3564\n",
            "Epoch 6 Batch 5100 Loss 1.6325 Accuracy 0.3564\n",
            "Epoch 6 Batch 5150 Loss 1.6329 Accuracy 0.3563\n",
            "Epoch 6 Batch 5200 Loss 1.6335 Accuracy 0.3563\n",
            "Epoch 6 Batch 5250 Loss 1.6342 Accuracy 0.3563\n",
            "Epoch 6 Batch 5300 Loss 1.6347 Accuracy 0.3563\n",
            "Epoch 6 Batch 5350 Loss 1.6352 Accuracy 0.3562\n",
            "Epoch 6 Batch 5400 Loss 1.6356 Accuracy 0.3562\n",
            "Epoch 6 Batch 5450 Loss 1.6361 Accuracy 0.3562\n",
            "Epoch 6 Batch 5500 Loss 1.6368 Accuracy 0.3561\n",
            "Epoch 6 Batch 5550 Loss 1.6374 Accuracy 0.3561\n",
            "Epoch 6 Batch 5600 Loss 1.6374 Accuracy 0.3561\n",
            "Epoch 6 Batch 5650 Loss 1.6376 Accuracy 0.3560\n",
            "Epoch 6 Batch 5700 Loss 1.6377 Accuracy 0.3559\n",
            "Epoch 6 Batch 5750 Loss 1.6378 Accuracy 0.3559\n",
            "Epoch 6 Batch 5800 Loss 1.6379 Accuracy 0.3558\n",
            "Epoch 6 Batch 5850 Loss 1.6381 Accuracy 0.3558\n",
            "Epoch 6 Batch 5900 Loss 1.6381 Accuracy 0.3557\n",
            "Epoch 6 Batch 5950 Loss 1.6379 Accuracy 0.3556\n",
            "Epoch 6 Batch 6000 Loss 1.6378 Accuracy 0.3555\n",
            "Epoch 6 Batch 6050 Loss 1.6377 Accuracy 0.3555\n",
            "Epoch 6 Batch 6100 Loss 1.6375 Accuracy 0.3554\n",
            "Saved checkpoint for epoch 6!\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.7638 Accuracy 0.3454\n",
            "Epoch 7 Batch 50 Loss 1.6795 Accuracy 0.3462\n",
            "Epoch 7 Batch 100 Loss 1.6874 Accuracy 0.3482\n",
            "Epoch 7 Batch 150 Loss 1.6875 Accuracy 0.3490\n",
            "Epoch 7 Batch 200 Loss 1.6822 Accuracy 0.3500\n",
            "Epoch 7 Batch 250 Loss 1.6806 Accuracy 0.3504\n",
            "Epoch 7 Batch 300 Loss 1.6810 Accuracy 0.3512\n",
            "Epoch 7 Batch 350 Loss 1.6789 Accuracy 0.3516\n",
            "Epoch 7 Batch 400 Loss 1.6742 Accuracy 0.3517\n",
            "Epoch 7 Batch 450 Loss 1.6693 Accuracy 0.3521\n",
            "Epoch 7 Batch 500 Loss 1.6650 Accuracy 0.3522\n",
            "Epoch 7 Batch 550 Loss 1.6626 Accuracy 0.3524\n",
            "Epoch 7 Batch 600 Loss 1.6595 Accuracy 0.3522\n",
            "Epoch 7 Batch 650 Loss 1.6557 Accuracy 0.3522\n",
            "Epoch 7 Batch 700 Loss 1.6549 Accuracy 0.3524\n",
            "Epoch 7 Batch 750 Loss 1.6565 Accuracy 0.3522\n",
            "Epoch 7 Batch 800 Loss 1.6588 Accuracy 0.3523\n",
            "Epoch 7 Batch 850 Loss 1.6612 Accuracy 0.3523\n",
            "Epoch 7 Batch 900 Loss 1.6614 Accuracy 0.3517\n",
            "Epoch 7 Batch 950 Loss 1.6625 Accuracy 0.3517\n",
            "Epoch 7 Batch 1000 Loss 1.6610 Accuracy 0.3515\n",
            "Epoch 7 Batch 1050 Loss 1.6613 Accuracy 0.3514\n",
            "Epoch 7 Batch 1100 Loss 1.6616 Accuracy 0.3514\n",
            "Epoch 7 Batch 1150 Loss 1.6600 Accuracy 0.3513\n",
            "Epoch 7 Batch 1200 Loss 1.6598 Accuracy 0.3512\n",
            "Epoch 7 Batch 1250 Loss 1.6594 Accuracy 0.3510\n",
            "Epoch 7 Batch 1300 Loss 1.6594 Accuracy 0.3510\n",
            "Epoch 7 Batch 1350 Loss 1.6591 Accuracy 0.3508\n",
            "Epoch 7 Batch 1400 Loss 1.6592 Accuracy 0.3506\n",
            "Epoch 7 Batch 1450 Loss 1.6591 Accuracy 0.3504\n",
            "Epoch 7 Batch 1500 Loss 1.6587 Accuracy 0.3504\n",
            "Epoch 7 Batch 1550 Loss 1.6582 Accuracy 0.3504\n",
            "Epoch 7 Batch 1600 Loss 1.6573 Accuracy 0.3504\n",
            "Epoch 7 Batch 1650 Loss 1.6560 Accuracy 0.3504\n",
            "Epoch 7 Batch 1700 Loss 1.6555 Accuracy 0.3504\n",
            "Epoch 7 Batch 1750 Loss 1.6539 Accuracy 0.3503\n",
            "Epoch 7 Batch 1800 Loss 1.6525 Accuracy 0.3502\n",
            "Epoch 7 Batch 1850 Loss 1.6503 Accuracy 0.3502\n",
            "Epoch 7 Batch 1900 Loss 1.6486 Accuracy 0.3501\n",
            "Epoch 7 Batch 1950 Loss 1.6473 Accuracy 0.3500\n",
            "Epoch 7 Batch 2000 Loss 1.6458 Accuracy 0.3499\n",
            "Epoch 7 Batch 2050 Loss 1.6443 Accuracy 0.3498\n",
            "Epoch 7 Batch 2100 Loss 1.6430 Accuracy 0.3498\n",
            "Epoch 7 Batch 2150 Loss 1.6410 Accuracy 0.3498\n",
            "Epoch 7 Batch 2200 Loss 1.6398 Accuracy 0.3497\n",
            "Epoch 7 Batch 2250 Loss 1.6382 Accuracy 0.3496\n",
            "Epoch 7 Batch 2300 Loss 1.6369 Accuracy 0.3496\n",
            "Epoch 7 Batch 2350 Loss 1.6353 Accuracy 0.3496\n",
            "Epoch 7 Batch 2400 Loss 1.6333 Accuracy 0.3497\n",
            "Epoch 7 Batch 2450 Loss 1.6316 Accuracy 0.3497\n",
            "Epoch 7 Batch 2500 Loss 1.6299 Accuracy 0.3499\n",
            "Epoch 7 Batch 2550 Loss 1.6269 Accuracy 0.3499\n",
            "Epoch 7 Batch 2600 Loss 1.6245 Accuracy 0.3501\n",
            "Epoch 7 Batch 2650 Loss 1.6230 Accuracy 0.3503\n",
            "Epoch 7 Batch 2700 Loss 1.6217 Accuracy 0.3506\n",
            "Epoch 7 Batch 2750 Loss 1.6206 Accuracy 0.3507\n",
            "Epoch 7 Batch 2800 Loss 1.6188 Accuracy 0.3510\n",
            "Epoch 7 Batch 2850 Loss 1.6175 Accuracy 0.3512\n",
            "Epoch 7 Batch 2900 Loss 1.6167 Accuracy 0.3514\n",
            "Epoch 7 Batch 2950 Loss 1.6156 Accuracy 0.3517\n",
            "Epoch 7 Batch 3000 Loss 1.6144 Accuracy 0.3520\n",
            "Epoch 7 Batch 3050 Loss 1.6137 Accuracy 0.3523\n",
            "Epoch 7 Batch 3100 Loss 1.6125 Accuracy 0.3526\n",
            "Epoch 7 Batch 3150 Loss 1.6116 Accuracy 0.3528\n",
            "Epoch 7 Batch 3200 Loss 1.6105 Accuracy 0.3531\n",
            "Epoch 7 Batch 3250 Loss 1.6098 Accuracy 0.3534\n",
            "Epoch 7 Batch 3300 Loss 1.6097 Accuracy 0.3537\n",
            "Epoch 7 Batch 3350 Loss 1.6094 Accuracy 0.3540\n",
            "Epoch 7 Batch 3400 Loss 1.6090 Accuracy 0.3543\n",
            "Epoch 7 Batch 3450 Loss 1.6083 Accuracy 0.3546\n",
            "Epoch 7 Batch 3500 Loss 1.6075 Accuracy 0.3549\n",
            "Epoch 7 Batch 3550 Loss 1.6065 Accuracy 0.3552\n",
            "Epoch 7 Batch 3600 Loss 1.6058 Accuracy 0.3555\n",
            "Epoch 7 Batch 3650 Loss 1.6053 Accuracy 0.3557\n",
            "Epoch 7 Batch 3700 Loss 1.6048 Accuracy 0.3560\n",
            "Epoch 7 Batch 3750 Loss 1.6041 Accuracy 0.3563\n",
            "Epoch 7 Batch 3800 Loss 1.6036 Accuracy 0.3565\n",
            "Epoch 7 Batch 3850 Loss 1.6026 Accuracy 0.3569\n",
            "Epoch 7 Batch 3900 Loss 1.6022 Accuracy 0.3572\n",
            "Epoch 7 Batch 3950 Loss 1.6020 Accuracy 0.3575\n",
            "Epoch 7 Batch 4000 Loss 1.6010 Accuracy 0.3578\n",
            "Epoch 7 Batch 4050 Loss 1.6006 Accuracy 0.3581\n",
            "Epoch 7 Batch 4100 Loss 1.6005 Accuracy 0.3584\n",
            "Epoch 7 Batch 4150 Loss 1.5998 Accuracy 0.3587\n",
            "Epoch 7 Batch 4200 Loss 1.5990 Accuracy 0.3590\n",
            "Epoch 7 Batch 4250 Loss 1.5982 Accuracy 0.3593\n",
            "Epoch 7 Batch 4300 Loss 1.5975 Accuracy 0.3596\n",
            "Epoch 7 Batch 4350 Loss 1.5973 Accuracy 0.3599\n",
            "Epoch 7 Batch 4400 Loss 1.5965 Accuracy 0.3601\n",
            "Epoch 7 Batch 4450 Loss 1.5965 Accuracy 0.3603\n",
            "Epoch 7 Batch 4500 Loss 1.5966 Accuracy 0.3604\n",
            "Epoch 7 Batch 4550 Loss 1.5973 Accuracy 0.3605\n",
            "Epoch 7 Batch 4600 Loss 1.5981 Accuracy 0.3606\n",
            "Epoch 7 Batch 4650 Loss 1.5989 Accuracy 0.3606\n",
            "Epoch 7 Batch 4700 Loss 1.6000 Accuracy 0.3606\n",
            "Epoch 7 Batch 4750 Loss 1.6005 Accuracy 0.3606\n",
            "Epoch 7 Batch 4800 Loss 1.6016 Accuracy 0.3606\n",
            "Epoch 7 Batch 4850 Loss 1.6022 Accuracy 0.3606\n",
            "Epoch 7 Batch 4900 Loss 1.6030 Accuracy 0.3605\n",
            "Epoch 7 Batch 4950 Loss 1.6039 Accuracy 0.3605\n",
            "Epoch 7 Batch 5000 Loss 1.6048 Accuracy 0.3605\n",
            "Epoch 7 Batch 5050 Loss 1.6049 Accuracy 0.3605\n",
            "Epoch 7 Batch 5100 Loss 1.6056 Accuracy 0.3605\n",
            "Epoch 7 Batch 5150 Loss 1.6064 Accuracy 0.3605\n",
            "Epoch 7 Batch 5200 Loss 1.6068 Accuracy 0.3605\n",
            "Epoch 7 Batch 5250 Loss 1.6072 Accuracy 0.3605\n",
            "Epoch 7 Batch 5300 Loss 1.6078 Accuracy 0.3604\n",
            "Epoch 7 Batch 5350 Loss 1.6083 Accuracy 0.3604\n",
            "Epoch 7 Batch 5400 Loss 1.6087 Accuracy 0.3603\n",
            "Epoch 7 Batch 5450 Loss 1.6091 Accuracy 0.3603\n",
            "Epoch 7 Batch 5500 Loss 1.6098 Accuracy 0.3602\n",
            "Epoch 7 Batch 5550 Loss 1.6101 Accuracy 0.3602\n",
            "Epoch 7 Batch 5600 Loss 1.6107 Accuracy 0.3601\n",
            "Epoch 7 Batch 5650 Loss 1.6109 Accuracy 0.3600\n",
            "Epoch 7 Batch 5700 Loss 1.6109 Accuracy 0.3599\n",
            "Epoch 7 Batch 5750 Loss 1.6111 Accuracy 0.3599\n",
            "Epoch 7 Batch 5800 Loss 1.6112 Accuracy 0.3598\n",
            "Epoch 7 Batch 5850 Loss 1.6114 Accuracy 0.3598\n",
            "Epoch 7 Batch 5900 Loss 1.6116 Accuracy 0.3597\n",
            "Epoch 7 Batch 5950 Loss 1.6116 Accuracy 0.3596\n",
            "Epoch 7 Batch 6000 Loss 1.6114 Accuracy 0.3596\n",
            "Epoch 7 Batch 6050 Loss 1.6113 Accuracy 0.3595\n",
            "Epoch 7 Batch 6100 Loss 1.6113 Accuracy 0.3595\n",
            "Saved checkpoint for epoch 7!\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.6390 Accuracy 0.3725\n",
            "Epoch 8 Batch 50 Loss 1.6809 Accuracy 0.3569\n",
            "Epoch 8 Batch 100 Loss 1.6613 Accuracy 0.3558\n",
            "Epoch 8 Batch 150 Loss 1.6648 Accuracy 0.3548\n",
            "Epoch 8 Batch 200 Loss 1.6574 Accuracy 0.3546\n",
            "Epoch 8 Batch 250 Loss 1.6492 Accuracy 0.3545\n",
            "Epoch 8 Batch 300 Loss 1.6480 Accuracy 0.3549\n",
            "Epoch 8 Batch 350 Loss 1.6457 Accuracy 0.3549\n",
            "Epoch 8 Batch 400 Loss 1.6418 Accuracy 0.3546\n",
            "Epoch 8 Batch 450 Loss 1.6428 Accuracy 0.3551\n",
            "Epoch 8 Batch 500 Loss 1.6421 Accuracy 0.3556\n",
            "Epoch 8 Batch 550 Loss 1.6400 Accuracy 0.3558\n",
            "Epoch 8 Batch 600 Loss 1.6394 Accuracy 0.3560\n",
            "Epoch 8 Batch 650 Loss 1.6389 Accuracy 0.3557\n",
            "Epoch 8 Batch 700 Loss 1.6398 Accuracy 0.3558\n",
            "Epoch 8 Batch 750 Loss 1.6407 Accuracy 0.3556\n",
            "Epoch 8 Batch 800 Loss 1.6430 Accuracy 0.3556\n",
            "Epoch 8 Batch 850 Loss 1.6437 Accuracy 0.3554\n",
            "Epoch 8 Batch 900 Loss 1.6427 Accuracy 0.3553\n",
            "Epoch 8 Batch 950 Loss 1.6426 Accuracy 0.3553\n",
            "Epoch 8 Batch 1000 Loss 1.6417 Accuracy 0.3552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "Dfi77D_E63DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(inp_sentence):\n",
        "    # turn the sentence to the tokenizer_encoded format [hi, bye] -> [241, 6]\n",
        "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    # expand dim on axis=0 to simulate the batch dimmension\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    #  make the ouput which starts with  and add that axis=0 for batch=0\n",
        "    output = tf.expand_dims([VOCAB_SIZE_DE-2], axis=0)\n",
        "\n",
        "    # the loop to predict the next word of output each time and output += it\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        #  put false because \n",
        "        # predictions = [btch_sz=1, seq_len(output_so_far), vocav_sz_de(the \n",
        "        # softmax values of each word, the higher the number the higher the \n",
        "        # probability for that word)]\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        #  take the last word of this prediction\n",
        "        prediction = predictions[:, -1:, :]\n",
        "        #  argmax to get the index of the most probable next word\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        #  the end of the sentence\n",
        "        if predicted_id == VOCAB_SIZE_DE-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        # add the new prediction to the last of the output\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    \n",
        "    \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "metadata": {
        "id": "EbvWMrBd63KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction**"
      ],
      "metadata": {
        "id": "ZHaVM88N679C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    # get rid of  and  if they exist\n",
        "    output = [i for i in output if i < VOCAB_SIZE_DE-2]\n",
        "    # decode indexes to words e.g. [241, 6] -> [hi, bye] \n",
        "    predicted_sentence = tokenizer_de.decode(output)\n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "metadata": {
        "id": "04_wodlh68DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is a great Day\")"
      ],
      "metadata": {
        "id": "Nx83A9LH7B8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7553f2-6997-40ee-9ec5-38985a422c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: This is a great Day\n",
            "Predicted translation: Das ist ein großer Tag des Gute.\n"
          ]
        }
      ]
    }
  ]
}